{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c89bf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nas/home/rehanahm/notebooks/testing_NER/csci_project/erc\n"
     ]
    }
   ],
   "source": [
    "%cd /nas/home/rehanahm/notebooks/testing_NER/csci_project/erc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52c13b",
   "metadata": {},
   "source": [
    "# Splitting Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4dd50e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from utils import get_num_classes, ErcTextDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28eb1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/transcript_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bc35de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['Session'] < 5]\n",
    "df_valid = df[(df['Session'] == 5) & (df['Utterance'] % 2 == 0)]\n",
    "df_test = df[(df['Session'] == 5) & (df['Utterance'] % 2 == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92c95c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_ls = list(df_train['Emotion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2862fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_emo(e):\n",
    "    return emo_ls.index(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "047e5f7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38941/1716027103.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['label'] = list(map(fn_emo,df_train['Emotion']))\n",
      "/tmp/ipykernel_38941/1716027103.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['label'] = list(map(fn_emo,df_test['Emotion']))\n",
      "/tmp/ipykernel_38941/1716027103.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_valid['label'] = list(map(fn_emo,df_valid['Emotion']))\n"
     ]
    }
   ],
   "source": [
    "df_train['label'] = list(map(fn_emo,df_train['Emotion']))\n",
    "df_test['label'] = list(map(fn_emo,df_test['Emotion']))\n",
    "df_valid['label'] = list(map(fn_emo,df_valid['Emotion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a57b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['Unnamed: 0','text','label']].to_csv('../data/train.csv')\n",
    "df_test[['Unnamed: 0','text','label']].to_csv('../data/test.csv')\n",
    "df_valid[['Unnamed: 0','text','label']].to_csv('../data/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98cea0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir model_custom_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c8d2ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nas/home/rehanahm/notebooks/testing_NER/csci_project/erc/model_custom_checkpoint\n"
     ]
    }
   ],
   "source": [
    "%cd model_custom_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c4eac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-25 20:14:18--  https://huggingface.co/tae898/emoberta-base/resolve/main/pytorch_model.bin\n",
      "Resolving huggingface.co (huggingface.co)... 34.200.173.213, 34.224.55.150, 34.197.58.156, ...\n",
      "Connecting to huggingface.co (huggingface.co)|34.200.173.213|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/tae898/emoberta-base/f89836cedb395f1321d50bb4358e6baefe94a4d24ae15727d00e21befb2fdd5f [following]\n",
      "--2022-03-25 20:14:18--  https://cdn-lfs.huggingface.co/tae898/emoberta-base/f89836cedb395f1321d50bb4358e6baefe94a4d24ae15727d00e21befb2fdd5f\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.226.219.110, 13.226.219.109, 13.226.219.35, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.226.219.110|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 498686637 (476M) [application/zip]\n",
      "Saving to: ‘pytorch_model.bin’\n",
      "\n",
      "100%[======================================>] 498,686,637 21.3MB/s   in 16s    \n",
      "\n",
      "2022-03-25 20:14:36 (29.0 MB/s) - ‘pytorch_model.bin’ saved [498686637/498686637]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://huggingface.co/tae898/emoberta-base/resolve/main/pytorch_model.bin\n",
    "# !wget https://huggingface.co/tae898/emoberta-base/resolve/main/tokenizer_config.json.json\n",
    "# !wget https://huggingface.co/tae898/emoberta-base/resolve/main/trainer_state.json\n",
    "# !wget https://huggingface.co/tae898/emoberta-base/resolve/main/tokenizer.json\n",
    "# !wget https://huggingface.co/tae898/emoberta-base/resolve/main/val-results.json\n",
    "# !wget https://huggingface.co/tae898/emoberta-base/resolve/main/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe183ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nas/home/rehanahm/notebooks/testing_NER/csci_project/erc\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1aa0e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer.json',\n",
       " 'pytorch_model.bin',\n",
       " 'config.json',\n",
       " 'trainer_state.json',\n",
       " 'val-results.json']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('model_custom_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "772913c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = '/nas/home/rehanahm/notebooks/testing_NER/csci_project/erc/model_custom_checkpoint'\n",
    "\n",
    "\n",
    "\n",
    "NUM_CLASSES = get_num_classes('IEMOCAP')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18af74",
   "metadata": {},
   "source": [
    "# Loading Raw Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed734c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 20:15:22.460 WARNING builder - _create_builder_config: Using custom data configuration default-306d838a230f35bc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /nas/home/rehanahm/.cache/huggingface/datasets/csv/default-306d838a230f35bc/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63ece1433f043ca9201b3a38e6e792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4de9c189a4c61ac20f2919c947066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /nas/home/rehanahm/.cache/huggingface/datasets/csv/default-306d838a230f35bc/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 20:15:23.129 WARNING builder - _create_builder_config: Using custom data configuration default-af67eea0e557a14c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /nas/home/rehanahm/.cache/huggingface/datasets/csv/default-af67eea0e557a14c/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bb3faacb584bf2aa10eb1b8fb74ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8673454e2fd745d2b1b8e8c217bb5355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /nas/home/rehanahm/.cache/huggingface/datasets/csv/default-af67eea0e557a14c/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 20:15:23.743 WARNING builder - _create_builder_config: Using custom data configuration default-3e5c71c536f5a0d7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /nas/home/rehanahm/.cache/huggingface/datasets/csv/default-3e5c71c536f5a0d7/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d330a230a697425aaa75ea512333e2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544f36cd9d55481297888df9e11662bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /nas/home/rehanahm/.cache/huggingface/datasets/csv/default-3e5c71c536f5a0d7/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('csv', data_files='../data/train.csv',split='train')\n",
    "test_dataset = load_dataset('csv', data_files='../data/test.csv',split = 'train')\n",
    "val_dataset = load_dataset('csv', data_files='../data/val.csv',split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebf2c075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4358cc37724e6da0f59cb6650e5d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f787d85b6145379be6b22a97b08378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3badbaac0afb4d55af2571a9e7ffcad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=True,truncation=True)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "attention_mask = tokenized_train['attention_mask']+tokenized_test['attention_mask']+tokenized_val['attention_mask']\n",
    "input_ids = tokenized_train['attention_mask']+tokenized_test['attention_mask']+tokenized_val['attention_mask']\n",
    "index = tokenized_train['Unnamed: 0']+tokenized_test['Unnamed: 0']+tokenized_val['Unnamed: 0']\n",
    "\n",
    "\n",
    "tokenized_train = tokenized_train.remove_columns(['Unnamed: 0','text'])\n",
    "tokenized_test = tokenized_test.remove_columns(['Unnamed: 0','text'])\n",
    "tokenized_val = tokenized_val.remove_columns(['Unnamed: 0','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f5cf371",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict = {'index':index,'input_ids':input_ids,'attention_mask':attention_mask}\n",
    "with open('../data/dump_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(embed_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb02fd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer2\")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "19ee1872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x7fc4589f3df0>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdeb73f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee9893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.1.\n",
      "***** Running training *****\n",
      "  Num examples = 5758\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1079' max='2160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1079/2160 24:41 < 24:47, 0.73 it/s, Epoch 1.50/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.967100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer2/checkpoint-500\n",
      "Configuration saved in test-trainer2/checkpoint-500/config.json\n",
      "Model weights saved in test-trainer2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer2/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer2/checkpoint-1000\n",
      "Configuration saved in test-trainer2/checkpoint-1000/config.json\n",
      "Model weights saved in test-trainer2/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer2/checkpoint-1000/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1657e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pushnotif import PushNotif\n",
    "handler = PushNotif(key = \"bTk06BrKuJF-RuhBtd_EvG\", event = \"Notifier\")\n",
    "handler.send(\"EXECUTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce87939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     './test-trainer/checkpoint-2000'\n",
    "# )\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89dcb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c525463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.1.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 810\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c4e0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd43c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e96af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [np.argmax(i) for i in predictions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b06af7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c01818b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    227\n",
       "3    190\n",
       "2    186\n",
       "0    123\n",
       "4     84\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(preds).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "158639df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    232\n",
       "3    184\n",
       "1    183\n",
       "0    123\n",
       "4     88\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53b18b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7197541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6555555555555556"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(preds, df_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0aefa48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a5b4850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         sad       0.69      0.69      0.69       123\n",
      "         neu       0.52      0.64      0.57       183\n",
      "         hap       0.84      0.68      0.75       232\n",
      "         fru       0.61      0.63      0.62       184\n",
      "         ang       0.67      0.64      0.65        88\n",
      "\n",
      "    accuracy                           0.66       810\n",
      "   macro avg       0.67      0.65      0.66       810\n",
      "weighted avg       0.67      0.66      0.66       810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_test['label'], preds, target_names=emo_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448cff31",
   "metadata": {},
   "source": [
    "# Bias Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "00c1a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are NOT allowed to use off-the-shelf fairness packages like ai360\n",
    "\n",
    "def stat_parity(preds, sens):\n",
    "    '''\n",
    "    :preds: numpy array of the model predictions. Consisting of 0s and 1s\n",
    "    :sens: numpy array of the sensitive features. Consisting of 0s and 1s\n",
    "    :return: the statistical parity. no need to take the absolute value\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame({'pred':preds,'prot':sens})\n",
    "\n",
    "    unpriv = len(df[(df['pred']==1)&(df['prot']==0)])/len(df[df['prot']==0])\n",
    "    priv = len(df[(df['pred']==1)&(df['prot']==1)])/len(df[df['prot']==1])\n",
    "    return priv - unpriv\n",
    "\n",
    "    \n",
    "    # TODO. 10pts\n",
    "\n",
    "\n",
    "def eq_oppo(preds, sens, labels):\n",
    "    tpr_1,tpr_2 = 0,0\n",
    "    '''\n",
    "    :preds: numpy array of the model predictions. Consisting of 0s and 1s\n",
    "    :sens: numpy array of the sensitive features. Consisting of 0s and 1s\n",
    "    :labels: numpy array of the ground truth labels of the outcome. Consisting of 0s and 1s\n",
    "    :return: the statistical parity. no need to take the absolute value\n",
    "    '''\n",
    "    df = pd.DataFrame({'pred':preds,'prot':sens,'labels':labels})\n",
    "    try:\n",
    "        a = len(df[(df['pred']==1) & (df['labels']==1) & (df['prot']==1)]) / len(df[(df['labels']==1) & (df['prot']==1)])\n",
    "    except:\n",
    "        a = 0\n",
    "    try:\n",
    "        b = len(df[(df['pred']==1) & (df['labels']==1) & (df['prot']==0)]) / len(df[(df['labels']==1) & (df['prot']==0)])\n",
    "    except:\n",
    "        b=0    \n",
    "    \n",
    "    return a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "223b5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = {'F':0,'M':1}\n",
    "sens = df_test['Gender'].apply(lambda a: dc[a])\n",
    "labels = df_test['label']\n",
    "# preds = preds\n",
    "bias_df = pd.DataFrame({'gender':sens,'label':labels,'pred':preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "240ce910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    425\n",
       "1    385\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb2436f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for b_l in bias_df['label'].unique():\n",
    "    print(b_l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db683fcd",
   "metadata": {},
   "source": [
    "# Emotion Wise Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "554f6946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEMALE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         sad       0.67      0.65      0.66        69\n",
      "         neu       0.46      0.59      0.51        92\n",
      "         hap       0.85      0.65      0.74       125\n",
      "         fru       0.61      0.65      0.63       100\n",
      "         ang       0.64      0.64      0.64        39\n",
      "\n",
      "    accuracy                           0.64       425\n",
      "   macro avg       0.65      0.64      0.64       425\n",
      "weighted avg       0.66      0.64      0.64       425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df = bias_df[bias_df['gender']==0]\n",
    "\n",
    "print(\"FEMALE\")\n",
    "print(classification_report(temp_df['label'], temp_df['pred'], target_names=emo_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "af52c564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MALE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         sad       0.71      0.74      0.73        54\n",
      "         neu       0.58      0.69      0.63        91\n",
      "         hap       0.84      0.71      0.77       107\n",
      "         fru       0.61      0.61      0.61        84\n",
      "         ang       0.69      0.63      0.66        49\n",
      "\n",
      "    accuracy                           0.68       385\n",
      "   macro avg       0.68      0.68      0.68       385\n",
      "weighted avg       0.69      0.68      0.68       385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df = bias_df[bias_df['gender']==1]\n",
    "\n",
    "print(\"MALE\")\n",
    "\n",
    "print(classification_report(temp_df['label'], temp_df['pred'], target_names=emo_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af8f31",
   "metadata": {},
   "source": [
    "# Statistical Parity & Equal Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "79aac30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\t0.01345938422892834 0.01219251336898386\n",
      "\n",
      "neu\t0.03572960715817863 -0.005469824293353742\n",
      "\n",
      "hap\t-0.007290167865707464 -0.012834224598930466\n",
      "\n",
      "fru\t0.01651929465882951 0.03122994652406419\n",
      "\n",
      "ang\t-0.00539723661485314 -0.02511841100076395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bias_df\n",
    "\n",
    "for emo in range(0,5):\n",
    "    print(emo_ls[emo],end= \"\\t\")\n",
    "    sens,labels,preds = bias_df['gender'],bias_df['label'].apply(lambda a:0 if a==emo else 1),bias_df['pred'].apply(lambda a:0 if a==emo else 1)\n",
    "\n",
    "    print(eq_oppo(preds, sens, labels), stat_parity(preds, sens))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bd68fac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(i==j) for i,j in zip(bias_df['label'],bias_df['pred'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "317c04c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    687\n",
       "0    123\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_df['label'].apply(lambda a:0 if a==0 else 1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5693bf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    232\n",
       "3    184\n",
       "1    183\n",
       "0    123\n",
       "4     88\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d812f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eb15377f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "303c2a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "7375    3\n",
       "7376    3\n",
       "7377    3\n",
       "7378    3\n",
       "7379    4\n",
       "Name: label, Length: 5758, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4300c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe39bae",
   "metadata": {},
   "source": [
    "# Saving Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6e160254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "7f582ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentence_embedding(sent):\n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "\n",
    "    token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "\n",
    "    with torch.no_grad():\n",
    "         output = model_embed(**encoded)\n",
    "\n",
    "\n",
    "    hidden_states = output['hidden_states']\n",
    "#     print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "    layer_i = 0\n",
    "\n",
    "#     print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "    batch_i = 0\n",
    "\n",
    "#     print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "    token_i = 0\n",
    "\n",
    "#     print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    \n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    \n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "#     print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "\n",
    "    # `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "    # `token_vecs` is a tensor with shape [22 x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "#     print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
    "    return sentence_embedding\n",
    "\n",
    "\n",
    "# token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f9b4cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed = AutoModelForSequenceClassification.from_pretrained(\n",
    "    './test-trainer2/checkpoint-2000',\n",
    "    output_hidden_states = True\n",
    ")\n",
    "embed_dict = pickle.load(open('../data/dump_embeddings.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f675cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict['text'] = list(df_train['text'])+list(df_test['text'])+list(df_valid['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "79050d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543870f896134390b409e39851bc00a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_embed.eval()\n",
    "embeds =[]\n",
    "for sent in tqdm(embed_dict['text']):\n",
    "    embeds.append(get_sentence_embedding(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3e0d68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict['embeddings'] = embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "6d3f1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dump_embeddings.pkl','wb+') as f:\n",
    "    pickle.dump(embed_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "5efb3a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.6907)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(embeds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "a22181de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "b27f631f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.5028e-01, -2.7225e-02,  1.7844e-01,  5.5549e-01,  1.1923e+00,\n",
       "         9.2259e-01, -1.0498e+00,  2.0631e-01,  1.0984e+00,  1.7576e-01,\n",
       "        -1.4359e-01, -1.0987e+00, -2.7227e-01, -1.0615e+00,  1.0146e+00,\n",
       "         1.0194e-01, -1.3428e+00, -3.2196e-01,  1.3269e-02,  9.1509e-01,\n",
       "        -1.3659e+00, -3.3974e-01, -2.5787e-01, -4.4873e-01,  2.6469e-01,\n",
       "         1.1638e+00, -6.3356e-01, -2.4429e-01,  1.6176e+00,  2.3976e-02,\n",
       "        -3.9200e-01,  6.2035e-01, -1.7817e-01, -7.0268e-01,  6.7816e-01,\n",
       "         2.1686e-01,  1.8320e-01, -1.6528e-01, -9.0290e-02, -2.3172e-01,\n",
       "         1.2763e-01, -8.9797e-01, -2.6279e-01, -1.3188e+00, -2.4802e-02,\n",
       "        -3.5177e-01,  8.8172e-01,  2.9082e-01,  1.9420e-01,  8.6537e-01,\n",
       "         1.8325e-01,  1.6114e-01,  8.1260e-01, -1.7920e-01,  2.7502e-01,\n",
       "         5.8187e-01, -5.7841e-01,  5.0530e-01, -7.5623e-01, -5.7425e-02,\n",
       "         2.1691e-01,  4.3689e-01, -1.3669e-01,  1.9610e-01,  3.0762e-01,\n",
       "        -1.4486e-01,  1.1787e+00, -9.3914e-01, -5.3090e-01, -5.0736e-01,\n",
       "         2.7852e-01, -7.4750e-02, -4.0518e-01, -1.2984e+00, -1.0815e-01,\n",
       "        -2.2770e+00, -1.0119e+00, -2.8569e+00, -1.0575e+00,  1.3328e-01,\n",
       "        -5.1939e-02,  1.3551e-02,  3.8118e-01,  1.1006e+00, -5.6991e-01,\n",
       "        -4.8613e-01, -2.8966e-01,  8.1126e-01,  3.4186e-01, -1.1038e+00,\n",
       "        -8.2860e-01,  5.1007e-01,  1.7010e-01,  3.2919e-01, -1.6666e-01,\n",
       "        -7.5386e-01, -1.3651e+00,  4.5840e-01,  1.7289e-01,  1.1039e+00,\n",
       "        -4.7935e-02, -3.7750e-01, -2.3373e-01,  5.0530e-02, -3.1197e-01,\n",
       "        -6.9284e-01,  2.9489e-01,  1.0525e+00,  1.0017e+00, -5.7482e-01,\n",
       "         6.1896e-01,  1.1677e+00,  1.3285e+00,  2.4386e-01, -4.7740e-01,\n",
       "         6.2231e-01, -3.1305e-01, -1.1732e+00,  2.1550e-01,  5.0605e-01,\n",
       "         9.6808e-01, -4.0674e-01,  1.0482e-01,  5.1629e-01,  4.0189e-01,\n",
       "        -3.4917e-03,  6.5055e-01, -8.2210e-01, -1.3576e-01, -3.8951e-01,\n",
       "        -1.3819e-01, -9.7703e-01,  5.8447e-01, -1.1054e+00,  6.9420e-02,\n",
       "         2.3770e-01, -9.7492e-01,  4.5471e-02, -2.0886e-01,  9.8902e-01,\n",
       "         1.2112e-01,  7.3511e-01,  6.8466e-01,  1.3157e-01,  4.3002e-01,\n",
       "        -3.9141e-01,  9.7218e-02, -1.3980e-01,  2.6849e-01,  8.1608e-01,\n",
       "         4.6639e-01,  3.9421e-02, -6.6718e-01, -5.4755e-01, -6.3887e-01,\n",
       "         7.5662e-01,  1.3145e+00,  5.4383e-01, -1.9156e-01,  3.2660e-02,\n",
       "        -1.9402e-01,  9.7625e-03, -3.2339e-01,  6.0139e-01,  4.2577e-01,\n",
       "         5.0267e-01, -5.1812e-01,  8.9046e-01, -1.5131e+00,  4.2977e-01,\n",
       "         2.4136e-02, -6.7035e-01, -3.6521e-01, -5.9649e-01, -3.6154e-01,\n",
       "        -1.0399e+00,  1.2254e-01,  9.2270e-01, -1.3593e-01, -1.6536e-02,\n",
       "         1.0776e+00,  2.5849e-01,  1.1193e-01, -3.8760e-01, -2.6580e-01,\n",
       "         3.0086e-01,  1.2801e+00,  9.7330e-01, -5.5892e-01, -6.0861e-01,\n",
       "         3.2767e-01, -3.1934e-01,  5.3492e-02, -2.9222e-01,  2.7699e-01,\n",
       "        -2.4905e-01,  4.8448e-01, -5.5251e-01,  1.0339e-01, -2.6577e-01,\n",
       "         1.0166e-02, -3.7204e-01, -6.4470e-01, -6.9412e-01,  9.5341e-01,\n",
       "         1.8160e-01, -6.0177e-02,  9.5954e-01, -1.1958e+00, -3.1214e-01,\n",
       "         6.2054e-01, -5.0443e-02,  3.5419e-01,  3.0962e-01, -1.0587e+00,\n",
       "         8.8652e-01, -2.3522e-01, -2.7257e-01, -8.2056e-01,  1.4941e-01,\n",
       "        -1.0576e-01,  9.6080e-01,  2.0556e-01,  4.4359e-01,  3.1297e-01,\n",
       "         3.8435e-02, -6.1000e-01,  5.9864e-01,  5.4678e-01, -3.2880e-01,\n",
       "         4.6335e-01, -7.0209e-01,  8.2909e-01,  7.0023e-01, -3.2305e-01,\n",
       "        -3.7141e-01, -3.6047e-01,  9.4590e-01,  1.3969e+00,  4.7425e-04,\n",
       "        -1.2747e-01,  1.9084e-01,  1.0016e+00, -2.6210e-01,  1.3253e-01,\n",
       "        -9.6731e-01,  4.7778e-01, -1.4079e+00, -1.4084e+00, -8.8250e-01,\n",
       "         3.5657e-01,  1.8807e-01,  3.6784e-01,  4.7399e-01,  1.1550e+00,\n",
       "         5.6847e-01, -3.4158e-01,  2.6342e-01,  4.0010e-01,  9.7071e-01,\n",
       "         1.1315e-01, -2.3246e-01, -3.1159e-01,  3.5793e-01,  3.2614e-01,\n",
       "        -3.8262e-01,  2.1764e-01, -6.9480e-01, -2.4385e-02, -7.7299e-02,\n",
       "         1.0380e+00,  1.9845e-01, -7.8006e-01, -2.3981e-03, -7.0978e-01,\n",
       "         4.5230e-01, -1.2801e+00, -9.3076e-01, -1.5628e+00,  8.6286e-01,\n",
       "        -4.5022e-01,  7.9653e-01,  1.4980e-01,  6.1424e-01,  3.6963e-01,\n",
       "         8.7412e-02, -7.8350e-01,  5.3052e-02,  7.7864e-02, -1.5725e-02,\n",
       "         6.9372e-01,  6.2306e-01, -8.3753e-01, -3.4283e-02,  4.1813e-01,\n",
       "         1.6583e+00, -6.1475e-01,  9.9050e-02,  4.8841e-01,  8.9012e-01,\n",
       "        -1.8414e+00, -1.5100e+00, -5.5288e-01, -1.0842e+00,  6.3277e-01,\n",
       "        -2.7094e-01,  1.0021e+00,  2.5721e-01,  5.0340e-01,  6.6278e-01,\n",
       "        -1.2116e-01,  3.0296e-01,  3.1484e-01,  6.4703e-02, -7.1942e-01,\n",
       "        -1.0169e+00,  4.3396e-01,  4.6529e-01,  5.1175e-01,  1.0057e+00,\n",
       "        -2.6065e-01,  3.4637e-01, -1.0115e+00, -4.2606e-03,  6.4842e-02,\n",
       "        -9.4208e-01, -1.4367e+00, -1.1649e-01,  1.2176e-02,  3.2061e-02,\n",
       "        -5.7040e-01,  5.2884e-01, -1.0478e+00,  8.2437e-01, -7.1527e-01,\n",
       "         4.0004e-01,  9.4396e-01,  1.3908e-01,  6.3633e-01,  1.4164e-01,\n",
       "        -1.2078e+00,  1.3612e-01, -4.6311e-01,  1.6287e-01,  1.3037e-01,\n",
       "        -6.2244e-02,  3.8060e-01, -8.5311e-01,  2.0416e-01,  4.4084e-02,\n",
       "        -8.6548e-01, -1.1790e-01, -4.9720e-01, -1.0055e+00, -2.3323e-01,\n",
       "         3.8034e-01,  4.8054e-01, -6.5338e-03,  5.9618e-01, -2.8462e+00,\n",
       "        -2.3650e-01,  1.8429e+00,  1.1807e+00, -2.2407e-01,  7.2897e-01,\n",
       "        -3.1647e-01, -8.7830e-01, -1.1106e+00, -3.4031e-01, -7.5188e-01,\n",
       "        -2.2872e-01, -3.5376e-01, -7.9055e-01,  6.7661e-01,  3.4168e-01,\n",
       "         5.9962e-01,  4.4060e-01,  6.0923e-01,  1.9326e-01,  2.5213e-01,\n",
       "         7.7118e-01,  7.6982e-01,  9.9540e-01,  6.8738e-01,  5.3360e-01,\n",
       "         7.7518e-01, -1.2036e-01,  1.6665e+00,  1.5314e+00,  1.0055e+00,\n",
       "        -6.0926e-02, -3.4011e-01, -1.0514e+00, -6.2521e-01, -5.8639e-01,\n",
       "         1.0450e+00, -6.8774e-01,  6.3963e-01,  1.4735e-01,  5.8109e-01,\n",
       "         4.1731e-02, -3.5759e-01,  7.0542e-01, -8.1662e-02, -9.7647e-01,\n",
       "        -7.7652e-01,  1.0171e+00,  2.9340e-01,  6.6289e-01, -9.8751e-01,\n",
       "        -1.2420e+00, -8.5292e-01,  6.1785e-02, -3.1652e-01,  2.8493e-01,\n",
       "         1.1928e+00,  1.1654e+00,  4.6734e-01, -1.1816e+00, -4.8510e-01,\n",
       "         2.8460e-01, -8.3296e-02, -6.0745e-01, -3.7553e-01,  3.9004e-01,\n",
       "        -6.7437e-02, -1.2968e-03,  1.6189e-01, -1.4569e+00, -9.0844e-01,\n",
       "         1.5430e-01,  5.9977e-01, -1.5056e+00, -3.5642e-02,  2.0968e-01,\n",
       "         1.4956e-01,  1.6762e+00, -1.3911e-01, -6.9393e-01,  4.2273e-01,\n",
       "         3.6430e-01,  1.3187e-01,  7.5743e-03,  1.2072e+00, -8.2469e-02,\n",
       "        -8.1626e-01,  1.8112e-01,  2.1425e-01, -1.0637e+00, -1.2777e+00,\n",
       "        -2.9958e-01, -6.5316e-01,  1.6452e-03, -2.6211e+00,  5.9793e-01,\n",
       "         3.0014e-01,  1.1651e-02, -1.1323e-01,  6.8410e-01,  1.9943e-01,\n",
       "        -7.8519e-01, -1.2171e+00,  5.4993e-01,  5.9450e-01,  4.9948e-01,\n",
       "         3.2081e-01, -2.9923e-01,  1.1101e+00, -5.5765e-01,  1.8880e-01,\n",
       "        -3.5466e-02,  5.4179e-01,  3.4119e-01, -6.5781e-02, -1.4377e+00,\n",
       "        -7.4751e-01,  1.3443e+00,  9.7267e-01,  4.5958e-01, -2.2742e-01,\n",
       "         2.5788e-01,  2.3699e-01,  3.2163e-01,  6.1441e-02, -7.7064e-01,\n",
       "        -2.3956e-01,  6.8663e-01,  4.4078e-02, -7.2217e-01, -4.3073e-01,\n",
       "        -1.9779e-01, -8.3735e-01,  1.2509e+00,  5.6305e-01, -1.1801e+00,\n",
       "         2.0632e-01, -3.0702e-01, -2.0944e+00,  2.6588e-01,  2.2024e-01,\n",
       "        -1.5097e+00, -3.6570e-01,  1.4120e+00, -7.1473e-01, -3.1157e-01,\n",
       "        -4.6349e-01, -2.3427e-02, -7.0861e-01, -9.4280e-01, -7.7121e-01,\n",
       "         3.6975e-01, -8.4478e-02,  6.3305e-01,  1.3621e+00,  4.3800e-01,\n",
       "        -1.9275e-01, -3.1926e-01, -4.1457e-01, -8.9066e-01,  9.2342e-01,\n",
       "        -3.0146e-01, -2.1555e-01,  3.8328e-01,  5.1713e-01,  3.5908e-01,\n",
       "         1.1656e+00,  7.6597e-01, -3.4403e-01, -5.3937e-01,  4.9268e-01,\n",
       "         8.2114e-01, -8.1602e-01,  7.8236e-01,  3.0376e-01, -2.9936e-01,\n",
       "         2.5628e-01, -1.5582e-01, -4.4226e-01,  7.5725e-01, -3.1761e-01,\n",
       "        -1.6163e-01,  3.0045e-01,  3.0176e-02, -3.7387e-01, -5.8798e-01,\n",
       "        -4.9751e-01, -7.4485e-01,  2.5237e-01, -5.7931e-01, -5.6277e-01,\n",
       "         4.6521e-01,  3.6742e-01,  4.1979e-01, -1.5886e-01, -7.1090e-02,\n",
       "         1.6348e-01,  7.2769e-01,  4.9482e-01,  2.9852e-02,  3.0140e-01,\n",
       "        -1.4689e-01,  8.5570e-01,  9.2581e-02,  2.3287e-01,  6.1485e-01,\n",
       "        -9.3425e-02, -1.5787e-01,  3.7026e-02,  1.2235e+00, -3.5488e-01,\n",
       "        -4.2800e-01,  4.1157e-01,  3.7055e-01, -1.9062e-01,  1.4748e-01,\n",
       "         2.4523e-01,  4.6213e-01,  1.6799e-01, -8.1639e-01,  9.5775e-01,\n",
       "         2.0315e-01,  1.3364e+00,  1.0240e+00,  6.7771e-01, -2.5149e-01,\n",
       "        -5.5052e-01, -6.5725e-01, -2.9410e-01,  1.1996e+01,  2.3719e-01,\n",
       "         3.0850e-01,  9.8168e-01, -4.5634e-01, -3.4024e-01,  6.9245e-01,\n",
       "         5.1312e-01, -2.0268e-01,  1.2558e-01, -3.9725e-01, -5.6129e-01,\n",
       "        -1.3124e+00, -4.2881e-01, -4.2417e-01, -9.4222e-01,  6.4273e-01,\n",
       "         2.7553e-01,  3.7940e-01, -4.6368e-01, -1.0838e+00, -5.3508e-02,\n",
       "        -7.0658e-01,  2.8167e+00,  5.2866e-01, -1.1476e+00,  4.8749e-01,\n",
       "        -8.1228e-01,  9.8194e-01,  1.2713e+00, -2.7791e-01, -6.9846e-02,\n",
       "         3.1236e-03, -4.6690e-02,  1.2062e+00, -1.1297e+00,  9.5936e-01,\n",
       "         1.4393e+00,  7.4214e-01,  1.1655e-01, -9.3903e-02,  6.0218e-01,\n",
       "         5.0426e-03,  4.0291e-01, -2.7155e-01, -1.7259e-01,  4.4115e-01,\n",
       "         5.4150e-01,  1.1339e-01, -1.5378e-01,  5.3109e-01, -6.3452e-02,\n",
       "        -4.9398e-01, -9.6525e-02, -8.2315e-02, -8.2380e-02, -2.1047e-01,\n",
       "        -1.2777e+00, -1.6583e-01, -2.4563e-01,  5.0593e-02, -1.7202e-01,\n",
       "        -7.3104e-01,  3.4414e-02, -5.9132e-01, -5.7809e-01,  1.3961e+00,\n",
       "         1.2437e+00, -4.0123e-01,  4.5136e-01, -7.2891e-01,  1.2655e-01,\n",
       "        -3.7094e-01,  5.5349e-01,  8.7316e-01, -6.9604e-01,  3.1749e-01,\n",
       "        -3.5227e-01, -9.4339e-01,  1.3399e+00,  6.4968e-02,  4.4472e-02,\n",
       "         9.2050e-01, -4.8341e-01,  4.8583e-01,  7.3679e-02,  2.4777e-01,\n",
       "        -3.8371e-02, -6.9014e-01, -5.7413e-01, -9.3290e-01,  5.5911e-01,\n",
       "        -9.0531e-01,  2.3356e-01,  2.7213e-01,  7.2389e-01, -3.1946e-01,\n",
       "        -3.2858e-01,  3.6765e-01,  4.5807e-01, -3.9383e-01,  6.1640e-01,\n",
       "        -1.6622e-01, -1.6424e-02,  2.7199e-01,  3.5926e-01,  2.0439e-01,\n",
       "         8.5761e-01, -4.9925e-01,  1.4657e+00,  5.3387e-01,  5.8821e-01,\n",
       "         9.0209e-01,  6.9877e-01, -3.8226e-01, -4.9858e-01, -8.0371e-03,\n",
       "         5.6173e-01,  6.4445e-02,  2.2967e-02, -1.2741e+00,  1.2271e+00,\n",
       "        -5.0421e-01,  8.7067e-01, -5.2676e-02,  3.5395e-01, -1.5097e-02,\n",
       "        -1.8844e-01, -4.8687e-01, -9.2979e-01, -6.7770e-02,  9.7318e-01,\n",
       "        -2.8344e-01, -1.8178e-01,  8.0175e-01, -8.1949e-01,  4.1416e-01,\n",
       "         1.0091e-01, -2.2352e-01, -1.4045e-01,  4.6145e-01, -5.9100e-02,\n",
       "         2.6665e-02, -1.3137e+00, -2.2185e-01,  1.4398e-02,  1.1273e+00,\n",
       "        -2.1497e-01,  4.0204e-01, -6.7079e-01,  6.9423e-01, -1.0337e+00,\n",
       "         2.2024e-01, -7.0322e-02,  1.4341e-01,  3.8881e-01, -1.4936e-01,\n",
       "         4.2229e-01, -2.5882e-01, -3.2708e-01, -1.5587e-01, -3.1491e-02,\n",
       "        -5.2022e-01,  9.2258e-01, -4.0631e-01, -1.1413e+00,  4.7558e-01,\n",
       "        -2.5803e-01,  9.5926e-01, -8.4526e-01,  1.0862e+00, -1.5879e+00,\n",
       "        -3.2436e-01,  3.9342e-01, -7.1163e-01,  2.1924e-01,  4.3712e-01,\n",
       "         8.4183e-01,  1.8594e-01,  2.4150e-01])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dict['embeddings'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "6bc376f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "-rw-rw-r--+ 1 rehanahm div21 28M Apr 10 04:02 ../data/dump_embeddings.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltrh ../data/dump_embeddings.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "178345d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.1753e-01,  7.1740e-01,  1.9311e-01,  1.4703e-01,  4.0916e-01,\n",
       "         1.1628e+00,  6.6178e-01,  2.3801e-02, -9.9653e-02, -6.0544e-02,\n",
       "        -2.2448e-01,  1.7581e-01,  6.5957e-01, -4.0459e-01, -9.6910e-02,\n",
       "        -6.9616e-01,  4.5281e-01, -8.6157e-01, -1.3240e-02,  2.0969e+00,\n",
       "         3.1397e-02, -1.3169e+00, -5.7897e-01, -4.5992e-01, -1.5242e+00,\n",
       "         1.4780e-01,  1.7647e-01,  3.9375e-02,  1.5078e-01,  1.7738e+00,\n",
       "         1.5572e-01, -7.6408e-01,  2.8126e-01,  2.4109e-01, -3.0096e-01,\n",
       "         2.0284e-01, -3.2424e-01,  7.9904e-01,  1.1046e+00, -8.2546e-01,\n",
       "         5.4125e-01,  6.3639e-01,  2.7381e-01,  8.4687e-01,  5.0385e-01,\n",
       "        -8.5032e-01,  6.1866e-02,  1.1222e+00,  1.5168e-01, -7.1361e-01,\n",
       "        -2.1125e-01,  7.3605e-01,  4.0123e-01, -4.7860e-01,  1.9189e+00,\n",
       "        -7.0153e-01, -3.0373e-01, -3.9389e-01, -5.4133e-01,  7.1282e-01,\n",
       "        -3.8879e-01,  1.1574e+00,  1.1314e+00, -1.6852e-02,  6.9457e-01,\n",
       "         4.7828e-01, -2.4999e-01, -1.7124e+00,  2.8958e-01,  8.0655e-01,\n",
       "         4.1655e-01,  1.5017e-01,  1.4695e+00, -2.9966e-01, -1.9395e-02,\n",
       "        -2.3686e-01,  1.9503e-01, -2.8592e+00, -8.1160e-01, -7.0055e-02,\n",
       "        -5.6087e-01, -5.2947e-01,  6.1159e-01, -3.4074e-01,  7.1122e-01,\n",
       "        -8.2900e-01,  5.7699e-01,  1.0548e+00, -9.2895e-01,  7.9034e-01,\n",
       "        -4.5381e-01, -3.8774e-02,  5.9078e-01,  1.0167e+00, -2.3308e-02,\n",
       "         7.7554e-01, -1.1148e+00,  9.3041e-01,  1.1791e+00, -4.4595e-01,\n",
       "         8.3699e-01, -1.2034e-01,  3.8025e-02, -2.4483e-01, -9.7516e-01,\n",
       "         1.9942e+00, -2.9975e-01, -1.4845e-03,  1.4606e-01, -1.4222e-01,\n",
       "        -4.4840e-02, -1.2168e-01, -7.9997e-02,  1.1099e+00, -1.3486e-01,\n",
       "        -1.9965e-01,  3.2257e-01,  4.2883e-01,  1.0128e-02,  9.8855e-01,\n",
       "        -4.0017e-01, -1.1846e+00,  9.2576e-02, -2.1919e-02, -5.6454e-01,\n",
       "         1.0732e+00,  4.1085e-01,  4.0878e-01, -1.0801e+00, -1.7919e-03,\n",
       "         2.9195e-01, -7.5150e-01,  6.4177e-01, -1.9687e+00, -1.2280e+00,\n",
       "        -8.7090e-01, -4.9109e-02,  5.6473e-01, -5.9150e-01, -5.4500e-01,\n",
       "         3.7039e-01,  1.5236e-01,  3.7523e-01, -6.2202e-01, -6.3353e-01,\n",
       "        -7.7205e-01,  1.1389e-02,  5.5528e-01, -4.4442e-01, -2.0255e-01,\n",
       "         1.7557e-01,  1.3205e+00,  3.6698e-01, -8.6418e-01, -5.6256e-01,\n",
       "         8.5686e-01,  4.8888e-01,  1.2232e+00,  4.8897e-01,  5.7830e-01,\n",
       "         2.4270e-01, -8.4590e-01, -7.2454e-01, -1.4527e+00, -1.5065e-01,\n",
       "         1.0061e+00, -6.0907e-01,  1.1628e-01, -2.2251e-01, -4.4549e-01,\n",
       "        -9.2179e-01, -9.5334e-01, -3.1199e-01, -1.7335e-02, -2.7249e-01,\n",
       "        -1.1941e+00,  1.2986e+00,  2.2240e-01,  2.7576e-01, -5.8130e-01,\n",
       "         2.7399e-01,  7.4938e-02,  9.1546e-01, -1.9023e-01, -5.7152e-01,\n",
       "         8.9894e-01,  1.3317e-01,  1.0957e+00,  3.0573e-01,  1.6426e-01,\n",
       "        -1.1754e-01, -1.8913e-01,  7.0793e-01,  7.2249e-01, -1.5945e-01,\n",
       "        -4.9877e-01,  6.9116e-01, -1.1201e+00, -2.7355e-01, -1.8727e-01,\n",
       "        -2.9270e-02,  2.7800e-01, -1.2998e-01, -4.8280e-01,  8.6397e-01,\n",
       "        -5.0244e-01, -1.1293e-01, -6.7247e-01, -1.6988e-01,  2.9640e-02,\n",
       "         5.3834e-02,  5.2601e-01, -5.3190e-01,  8.4411e-01, -1.8075e-01,\n",
       "        -3.5845e-01, -3.8866e-01,  7.4876e-01, -2.6634e-01,  8.2905e-02,\n",
       "         1.3537e+00, -4.0271e-01,  1.5115e-01, -4.3613e-01, -2.5049e-01,\n",
       "         7.0901e-01,  1.5734e+00,  4.2447e-01,  1.0838e+00,  5.0161e-01,\n",
       "        -9.4670e-02,  4.2808e-01, -6.2393e-01,  6.9012e-05, -1.1585e-01,\n",
       "         1.1759e+00, -2.5118e-01,  2.7207e-01, -5.9981e-01,  3.6404e-02,\n",
       "        -1.2897e+00,  3.0305e-01, -1.5175e-01, -1.2251e+00,  1.3786e-01,\n",
       "         1.4093e+00, -5.6863e-01,  2.4469e-01,  3.1799e-01, -3.9819e-01,\n",
       "        -6.1040e-01,  1.7710e-01,  7.3292e-04, -1.3681e+00,  4.8526e-02,\n",
       "        -7.9912e-01, -9.5191e-01, -5.6374e-01, -7.0021e-02, -1.1800e+00,\n",
       "        -7.8691e-01, -1.1629e-01,  1.2997e-01,  6.5424e-01, -3.7310e-01,\n",
       "        -5.7883e-01, -2.5238e-01, -6.7937e-01, -5.3099e-01, -1.0893e+00,\n",
       "        -3.2100e-01,  8.2277e-01, -8.9131e-01,  1.8192e-01, -1.5511e-01,\n",
       "        -8.7992e-01,  1.1700e+00, -4.6800e-02, -4.4717e-01, -3.3957e-01,\n",
       "        -1.7883e-01,  2.4106e-01,  1.6364e+00, -1.0545e+00, -2.8031e-01,\n",
       "         4.3591e-02,  1.1758e+00,  5.8924e-02,  1.6968e-01,  3.9515e-01,\n",
       "        -6.2849e-01,  6.4745e-02, -5.5966e-01,  6.0192e-01,  3.2315e-01,\n",
       "         1.3509e+00,  9.4696e-02,  9.3400e-02,  3.8213e-01,  3.9646e-01,\n",
       "        -1.3051e+00,  3.2701e-01,  5.1859e-01,  5.6182e-01, -3.7969e-01,\n",
       "        -5.6757e-01, -2.1071e-01, -1.3829e-01,  1.0882e+00, -3.2275e-01,\n",
       "         1.2286e-01,  2.2686e-01,  1.1858e-01,  9.7642e-01, -7.9653e-01,\n",
       "         3.3000e-01,  5.5361e-01, -4.7093e-01,  4.8933e-02, -8.3000e-02,\n",
       "         5.5471e-01, -2.6554e-01, -2.3005e-02,  5.5300e-01, -6.2087e-01,\n",
       "        -1.3333e+00,  1.1041e+00,  9.4307e-01,  4.9996e-01, -1.3479e+00,\n",
       "         3.4699e-02, -1.7644e-01, -9.9699e-01,  5.2025e-01,  7.9716e-01,\n",
       "         1.1381e+00,  3.7755e-01,  5.5861e-01,  2.2827e-01, -4.4761e-01,\n",
       "         8.8549e-01, -7.9046e-02,  1.7978e-01,  3.9664e-01, -3.7871e-01,\n",
       "         4.0073e-02, -6.9064e-02,  1.3956e-01,  3.8961e-01, -3.2285e-01,\n",
       "         1.4838e-01, -3.0027e-01, -1.0538e+00, -6.7407e-01, -2.4474e-01,\n",
       "        -1.8581e+00,  7.5340e-01,  1.1778e+00,  3.3145e-01, -7.8669e-01,\n",
       "        -1.6696e-01,  1.4769e+00, -1.9850e-01, -7.8277e-01, -6.6289e-01,\n",
       "        -1.0873e+00, -8.4113e-01, -4.8687e-02,  6.2696e-02, -1.0401e+00,\n",
       "        -1.1958e-01, -1.5392e-01,  2.6640e-01, -5.6878e-01, -6.9145e-01,\n",
       "         3.3936e-01,  6.6655e-01, -3.1648e-01,  1.6394e+00, -7.7151e-01,\n",
       "        -9.8968e-02,  1.7154e-01, -4.4553e-01,  1.2796e+00, -6.6693e-01,\n",
       "         7.3725e-02, -1.6464e-01, -1.5776e-01,  4.9989e-01, -9.3374e-01,\n",
       "        -1.7292e+00, -1.1459e+00,  2.0016e-01,  1.8506e-01,  4.5528e-01,\n",
       "         2.3421e-01,  5.2743e-01,  3.4280e-01,  2.5900e-01,  1.1510e+00,\n",
       "         1.2994e+00, -4.4975e-03, -3.9973e-01,  1.0146e-01, -7.9774e-01,\n",
       "        -2.9322e-01, -3.7247e-01,  2.0537e-01,  1.2269e+00,  1.6303e+00,\n",
       "         1.0863e+00,  2.8559e-01,  5.3299e-01, -1.2991e+00, -9.9205e-01,\n",
       "         9.5560e-01,  1.2615e+00, -1.7697e+00,  1.7203e-01, -1.2391e+00,\n",
       "        -8.6639e-01, -4.0946e-01,  4.5759e-01,  1.0261e+00,  1.0918e+00,\n",
       "        -2.3271e-01,  1.1240e+00, -1.0140e+00, -1.9496e-01, -1.4353e-01,\n",
       "        -4.4165e-01, -1.8401e-01,  9.3766e-01, -3.9427e-02, -1.3704e-01,\n",
       "        -6.0531e-01,  8.0319e-01,  8.2091e-01, -5.8684e-01,  8.3242e-01,\n",
       "        -1.3383e-01,  4.9070e-01,  5.3959e-01, -2.8478e-02,  5.1825e-01,\n",
       "         8.5146e-02,  8.7403e-01,  5.2693e-01, -3.4518e-01, -4.6001e-01,\n",
       "        -5.5545e-01,  1.4574e+00, -5.6648e-01, -2.3662e+00, -8.5848e-01,\n",
       "         5.1365e-01,  1.0420e+00,  7.5950e-01,  5.2747e-01, -1.6660e-01,\n",
       "         3.3883e-01,  1.4841e-01, -3.8542e-01,  1.5135e+00, -9.8802e-01,\n",
       "        -5.0428e-01,  3.3549e-01,  7.1296e-01, -7.5771e-03, -4.3516e-01,\n",
       "        -3.9713e-01,  7.0153e-01, -6.2398e-01,  1.8760e+00,  5.1917e-01,\n",
       "        -2.8491e-01, -8.5080e-01,  5.1622e-01,  4.6939e-01, -7.3797e-01,\n",
       "        -4.5611e-02, -3.6540e-01, -4.3079e-02,  1.0471e-01, -2.0622e-01,\n",
       "        -7.3532e-02, -1.8665e-01, -2.3531e-01, -5.9155e-01,  7.2684e-01,\n",
       "        -1.1098e-01,  1.3432e-01, -6.9938e-01,  1.2407e-01, -1.1405e+00,\n",
       "        -3.1078e-01, -9.5807e-01, -9.4654e-01,  8.0318e-02, -7.3391e-01,\n",
       "         3.1964e-01,  8.3654e-01, -1.2049e+00, -1.9618e+00, -2.7249e-01,\n",
       "        -4.6587e-01, -1.0404e+00,  8.8438e-01,  2.9090e-01,  4.3688e-01,\n",
       "        -1.4658e-01,  9.2043e-01, -9.9384e-01, -5.9380e-01,  4.2479e-01,\n",
       "        -1.9439e-01, -2.8628e-01, -2.1402e-02, -1.3438e+00,  8.8462e-01,\n",
       "         1.4709e-02,  1.1436e+00, -1.4277e+00,  3.1684e-01, -9.1904e-01,\n",
       "        -3.3638e-01, -2.2418e-01, -4.4777e-01, -3.3078e-02,  5.3539e-01,\n",
       "         7.0129e-01,  6.3903e-01, -4.8787e-01,  3.2388e-02,  3.5491e-02,\n",
       "        -6.6510e-01,  5.9044e-01,  1.7909e+00,  5.1809e-01,  1.1750e-01,\n",
       "         7.1087e-02, -3.4255e-01,  3.4289e-01,  1.8574e-01, -1.2857e+00,\n",
       "         1.3725e+00, -1.5392e-01, -5.2583e-01,  4.0612e-01, -2.7491e-01,\n",
       "         8.5352e-01,  3.0803e-01, -3.2295e-02, -3.9782e-01,  2.1039e-01,\n",
       "        -8.2026e-01,  8.7979e-02, -6.4619e-01,  4.0005e-01, -1.9427e-01,\n",
       "         8.6577e-01,  7.8731e-01,  3.7168e-02,  6.4830e-01,  7.1555e-01,\n",
       "         1.0375e+00,  8.8979e-01, -6.9319e-01,  2.9863e-01,  4.2301e-01,\n",
       "         1.0951e+00, -5.3058e-02, -8.0901e-01,  2.5248e-01, -7.3749e-01,\n",
       "         2.0375e-01,  1.1061e+00,  4.6453e-01,  9.7679e-01,  5.4552e-01,\n",
       "         1.1804e+00,  8.1731e-01, -7.7408e-01, -6.6645e-01, -1.0044e-01,\n",
       "         7.2182e-01, -5.1065e-01,  8.0432e-01,  1.0666e+01, -1.4236e-01,\n",
       "         5.4055e-01,  4.9012e-01, -2.7187e-01, -1.6978e-01, -2.0457e-01,\n",
       "        -4.8294e-01,  3.5407e-01,  1.0007e+00, -7.0252e-01,  1.1573e+00,\n",
       "         2.0560e-01,  3.1403e-01,  7.0654e-01, -1.1987e+00, -5.6500e-01,\n",
       "        -3.1729e-01, -3.4375e-01, -7.8753e-02,  6.0546e-01, -3.1423e-01,\n",
       "         4.7476e-01,  2.0691e+00,  1.0001e+00, -2.4814e-01,  7.7639e-01,\n",
       "        -4.9628e-01, -1.3472e+00, -1.0751e-01, -7.2293e-01,  5.7120e-01,\n",
       "        -1.1552e+00, -1.6819e-01,  3.0757e-01, -9.9463e-01, -9.1859e-01,\n",
       "        -3.6539e-01, -2.8873e-01,  1.2735e-01, -1.3904e+00,  1.8275e-01,\n",
       "         2.0644e-01, -3.2295e-01, -4.7965e-01,  3.1337e-01,  5.2009e-01,\n",
       "         7.8816e-02, -1.0510e+00, -1.4837e-01,  6.1726e-02,  3.5979e-01,\n",
       "        -1.2592e-01, -5.1714e-01, -1.1954e+00, -5.1691e-01, -9.3273e-01,\n",
       "        -5.9750e-01,  1.0698e+00,  1.2219e-01,  6.6855e-01,  2.0459e-01,\n",
       "         3.2773e-01,  1.1366e+00,  9.0465e-02,  3.4764e-01,  8.5130e-01,\n",
       "         6.5607e-01,  2.9318e-02,  1.0802e+00, -5.2018e-01,  1.2786e+00,\n",
       "         5.7886e-01,  1.0025e+00,  1.6957e+00, -5.5013e-01, -7.8420e-01,\n",
       "         4.7881e-01,  4.6326e-01, -4.5445e-01,  1.2226e+00, -7.4265e-01,\n",
       "        -5.7786e-01, -3.7873e-02,  4.4298e-02,  1.0396e+00, -1.0486e-01,\n",
       "        -1.7282e-01, -1.8327e+00, -1.5171e+00,  3.2456e-01,  2.6687e-02,\n",
       "         1.6264e+00,  4.4142e-01,  1.0567e+00,  8.4220e-01,  5.0290e-02,\n",
       "        -6.5854e-01,  5.1457e-01, -2.0974e-01,  5.1837e-01, -5.0703e-01,\n",
       "        -4.9442e-01, -9.8029e-02,  7.9233e-01, -7.9372e-01, -1.6354e-01,\n",
       "        -6.2970e-01, -6.6033e-01,  3.4697e-01, -1.5432e-01, -9.0975e-02,\n",
       "         1.9234e-02,  7.8202e-01, -2.9870e-01,  4.0201e-01,  4.5567e-01,\n",
       "        -3.5405e-01,  1.2441e+00,  9.6379e-01, -8.4405e-01,  2.9619e-01,\n",
       "        -8.9962e-02, -5.2986e-01,  7.9059e-01, -3.3404e-01,  7.2697e-01,\n",
       "        -1.1430e+00, -2.5904e-01, -1.2491e+00, -5.8071e-01,  4.3806e-01,\n",
       "        -7.3376e-01, -6.0002e-04, -4.2413e-02, -4.8845e-01, -1.8715e-01,\n",
       "         8.2170e-01,  3.8500e-01,  9.1317e-01, -4.7394e-01,  1.2065e+00,\n",
       "        -9.6900e-01, -1.8399e+00, -7.0725e-01, -2.8954e-01,  5.1278e-02,\n",
       "        -3.9569e-01, -1.3380e-01, -5.3554e-02,  9.5261e-02, -3.4867e-01,\n",
       "        -3.3921e-02,  3.1669e-01, -8.9116e-02,  7.9923e-01,  1.3674e-01,\n",
       "        -6.2165e-01,  6.2186e-01, -5.2999e-02,  2.0626e-01,  6.9465e-02,\n",
       "        -4.0762e-01,  7.6715e-02,  9.4340e-01, -6.7152e-01,  6.2884e-01,\n",
       "        -8.3146e-01, -4.3320e-01, -5.9787e-01,  1.0616e-01, -7.0941e-02,\n",
       "        -6.8430e-02,  6.3902e-01,  5.8441e-02,  4.4241e-01, -4.4279e-01,\n",
       "         3.8826e-01, -1.9156e-01,  8.4431e-02])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd91893",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "1cd22099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 13 x 768\n",
      "Our final sentence embedding vector of shape: torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.5028e-01, -2.7225e-02,  1.7844e-01,  5.5549e-01,  1.1923e+00,\n",
       "         9.2259e-01, -1.0498e+00,  2.0631e-01,  1.0984e+00,  1.7576e-01,\n",
       "        -1.4359e-01, -1.0987e+00, -2.7227e-01, -1.0615e+00,  1.0146e+00,\n",
       "         1.0194e-01, -1.3428e+00, -3.2196e-01,  1.3269e-02,  9.1509e-01,\n",
       "        -1.3659e+00, -3.3974e-01, -2.5787e-01, -4.4873e-01,  2.6469e-01,\n",
       "         1.1638e+00, -6.3356e-01, -2.4429e-01,  1.6176e+00,  2.3976e-02,\n",
       "        -3.9200e-01,  6.2035e-01, -1.7817e-01, -7.0268e-01,  6.7816e-01,\n",
       "         2.1686e-01,  1.8320e-01, -1.6528e-01, -9.0290e-02, -2.3172e-01,\n",
       "         1.2763e-01, -8.9797e-01, -2.6279e-01, -1.3188e+00, -2.4802e-02,\n",
       "        -3.5177e-01,  8.8172e-01,  2.9082e-01,  1.9420e-01,  8.6537e-01,\n",
       "         1.8325e-01,  1.6114e-01,  8.1260e-01, -1.7920e-01,  2.7502e-01,\n",
       "         5.8187e-01, -5.7841e-01,  5.0530e-01, -7.5623e-01, -5.7425e-02,\n",
       "         2.1691e-01,  4.3689e-01, -1.3669e-01,  1.9610e-01,  3.0762e-01,\n",
       "        -1.4486e-01,  1.1787e+00, -9.3914e-01, -5.3090e-01, -5.0736e-01,\n",
       "         2.7852e-01, -7.4750e-02, -4.0518e-01, -1.2984e+00, -1.0815e-01,\n",
       "        -2.2770e+00, -1.0119e+00, -2.8569e+00, -1.0575e+00,  1.3328e-01,\n",
       "        -5.1939e-02,  1.3551e-02,  3.8118e-01,  1.1006e+00, -5.6991e-01,\n",
       "        -4.8613e-01, -2.8966e-01,  8.1126e-01,  3.4186e-01, -1.1038e+00,\n",
       "        -8.2860e-01,  5.1007e-01,  1.7010e-01,  3.2919e-01, -1.6666e-01,\n",
       "        -7.5386e-01, -1.3651e+00,  4.5840e-01,  1.7289e-01,  1.1039e+00,\n",
       "        -4.7935e-02, -3.7750e-01, -2.3373e-01,  5.0530e-02, -3.1197e-01,\n",
       "        -6.9284e-01,  2.9489e-01,  1.0525e+00,  1.0017e+00, -5.7482e-01,\n",
       "         6.1896e-01,  1.1677e+00,  1.3285e+00,  2.4386e-01, -4.7740e-01,\n",
       "         6.2231e-01, -3.1305e-01, -1.1732e+00,  2.1550e-01,  5.0605e-01,\n",
       "         9.6808e-01, -4.0674e-01,  1.0482e-01,  5.1629e-01,  4.0189e-01,\n",
       "        -3.4917e-03,  6.5055e-01, -8.2210e-01, -1.3576e-01, -3.8951e-01,\n",
       "        -1.3819e-01, -9.7703e-01,  5.8447e-01, -1.1054e+00,  6.9420e-02,\n",
       "         2.3770e-01, -9.7492e-01,  4.5471e-02, -2.0886e-01,  9.8902e-01,\n",
       "         1.2112e-01,  7.3511e-01,  6.8466e-01,  1.3157e-01,  4.3002e-01,\n",
       "        -3.9141e-01,  9.7218e-02, -1.3980e-01,  2.6849e-01,  8.1608e-01,\n",
       "         4.6639e-01,  3.9421e-02, -6.6718e-01, -5.4755e-01, -6.3887e-01,\n",
       "         7.5662e-01,  1.3145e+00,  5.4383e-01, -1.9156e-01,  3.2660e-02,\n",
       "        -1.9402e-01,  9.7625e-03, -3.2339e-01,  6.0139e-01,  4.2577e-01,\n",
       "         5.0267e-01, -5.1812e-01,  8.9046e-01, -1.5131e+00,  4.2977e-01,\n",
       "         2.4136e-02, -6.7035e-01, -3.6521e-01, -5.9649e-01, -3.6154e-01,\n",
       "        -1.0399e+00,  1.2254e-01,  9.2270e-01, -1.3593e-01, -1.6536e-02,\n",
       "         1.0776e+00,  2.5849e-01,  1.1193e-01, -3.8760e-01, -2.6580e-01,\n",
       "         3.0086e-01,  1.2801e+00,  9.7330e-01, -5.5892e-01, -6.0861e-01,\n",
       "         3.2767e-01, -3.1934e-01,  5.3492e-02, -2.9222e-01,  2.7699e-01,\n",
       "        -2.4905e-01,  4.8448e-01, -5.5251e-01,  1.0339e-01, -2.6577e-01,\n",
       "         1.0166e-02, -3.7204e-01, -6.4470e-01, -6.9412e-01,  9.5341e-01,\n",
       "         1.8160e-01, -6.0177e-02,  9.5954e-01, -1.1958e+00, -3.1214e-01,\n",
       "         6.2054e-01, -5.0443e-02,  3.5419e-01,  3.0962e-01, -1.0587e+00,\n",
       "         8.8652e-01, -2.3522e-01, -2.7257e-01, -8.2056e-01,  1.4941e-01,\n",
       "        -1.0576e-01,  9.6080e-01,  2.0556e-01,  4.4359e-01,  3.1297e-01,\n",
       "         3.8435e-02, -6.1000e-01,  5.9864e-01,  5.4678e-01, -3.2880e-01,\n",
       "         4.6335e-01, -7.0209e-01,  8.2909e-01,  7.0023e-01, -3.2305e-01,\n",
       "        -3.7141e-01, -3.6047e-01,  9.4590e-01,  1.3969e+00,  4.7425e-04,\n",
       "        -1.2747e-01,  1.9084e-01,  1.0016e+00, -2.6210e-01,  1.3253e-01,\n",
       "        -9.6731e-01,  4.7778e-01, -1.4079e+00, -1.4084e+00, -8.8250e-01,\n",
       "         3.5657e-01,  1.8807e-01,  3.6784e-01,  4.7399e-01,  1.1550e+00,\n",
       "         5.6847e-01, -3.4158e-01,  2.6342e-01,  4.0010e-01,  9.7071e-01,\n",
       "         1.1315e-01, -2.3246e-01, -3.1159e-01,  3.5793e-01,  3.2614e-01,\n",
       "        -3.8262e-01,  2.1764e-01, -6.9480e-01, -2.4385e-02, -7.7299e-02,\n",
       "         1.0380e+00,  1.9845e-01, -7.8006e-01, -2.3981e-03, -7.0978e-01,\n",
       "         4.5230e-01, -1.2801e+00, -9.3076e-01, -1.5628e+00,  8.6286e-01,\n",
       "        -4.5022e-01,  7.9653e-01,  1.4980e-01,  6.1424e-01,  3.6963e-01,\n",
       "         8.7412e-02, -7.8350e-01,  5.3052e-02,  7.7864e-02, -1.5725e-02,\n",
       "         6.9372e-01,  6.2306e-01, -8.3753e-01, -3.4283e-02,  4.1813e-01,\n",
       "         1.6583e+00, -6.1475e-01,  9.9050e-02,  4.8841e-01,  8.9012e-01,\n",
       "        -1.8414e+00, -1.5100e+00, -5.5288e-01, -1.0842e+00,  6.3277e-01,\n",
       "        -2.7094e-01,  1.0021e+00,  2.5721e-01,  5.0340e-01,  6.6278e-01,\n",
       "        -1.2116e-01,  3.0296e-01,  3.1484e-01,  6.4703e-02, -7.1942e-01,\n",
       "        -1.0169e+00,  4.3396e-01,  4.6529e-01,  5.1175e-01,  1.0057e+00,\n",
       "        -2.6065e-01,  3.4637e-01, -1.0115e+00, -4.2606e-03,  6.4842e-02,\n",
       "        -9.4208e-01, -1.4367e+00, -1.1649e-01,  1.2176e-02,  3.2061e-02,\n",
       "        -5.7040e-01,  5.2884e-01, -1.0478e+00,  8.2437e-01, -7.1527e-01,\n",
       "         4.0004e-01,  9.4396e-01,  1.3908e-01,  6.3633e-01,  1.4164e-01,\n",
       "        -1.2078e+00,  1.3612e-01, -4.6311e-01,  1.6287e-01,  1.3037e-01,\n",
       "        -6.2244e-02,  3.8060e-01, -8.5311e-01,  2.0416e-01,  4.4084e-02,\n",
       "        -8.6548e-01, -1.1790e-01, -4.9720e-01, -1.0055e+00, -2.3323e-01,\n",
       "         3.8034e-01,  4.8054e-01, -6.5338e-03,  5.9618e-01, -2.8462e+00,\n",
       "        -2.3650e-01,  1.8429e+00,  1.1807e+00, -2.2407e-01,  7.2897e-01,\n",
       "        -3.1647e-01, -8.7830e-01, -1.1106e+00, -3.4031e-01, -7.5188e-01,\n",
       "        -2.2872e-01, -3.5376e-01, -7.9055e-01,  6.7661e-01,  3.4168e-01,\n",
       "         5.9962e-01,  4.4060e-01,  6.0923e-01,  1.9326e-01,  2.5213e-01,\n",
       "         7.7118e-01,  7.6982e-01,  9.9540e-01,  6.8738e-01,  5.3360e-01,\n",
       "         7.7518e-01, -1.2036e-01,  1.6665e+00,  1.5314e+00,  1.0055e+00,\n",
       "        -6.0926e-02, -3.4011e-01, -1.0514e+00, -6.2521e-01, -5.8639e-01,\n",
       "         1.0450e+00, -6.8774e-01,  6.3963e-01,  1.4735e-01,  5.8109e-01,\n",
       "         4.1731e-02, -3.5759e-01,  7.0542e-01, -8.1662e-02, -9.7647e-01,\n",
       "        -7.7652e-01,  1.0171e+00,  2.9340e-01,  6.6289e-01, -9.8751e-01,\n",
       "        -1.2420e+00, -8.5292e-01,  6.1785e-02, -3.1652e-01,  2.8493e-01,\n",
       "         1.1928e+00,  1.1654e+00,  4.6734e-01, -1.1816e+00, -4.8510e-01,\n",
       "         2.8460e-01, -8.3296e-02, -6.0745e-01, -3.7553e-01,  3.9004e-01,\n",
       "        -6.7437e-02, -1.2968e-03,  1.6189e-01, -1.4569e+00, -9.0844e-01,\n",
       "         1.5430e-01,  5.9977e-01, -1.5056e+00, -3.5642e-02,  2.0968e-01,\n",
       "         1.4956e-01,  1.6762e+00, -1.3911e-01, -6.9393e-01,  4.2273e-01,\n",
       "         3.6430e-01,  1.3187e-01,  7.5743e-03,  1.2072e+00, -8.2469e-02,\n",
       "        -8.1626e-01,  1.8112e-01,  2.1425e-01, -1.0637e+00, -1.2777e+00,\n",
       "        -2.9958e-01, -6.5316e-01,  1.6452e-03, -2.6211e+00,  5.9793e-01,\n",
       "         3.0014e-01,  1.1651e-02, -1.1323e-01,  6.8410e-01,  1.9943e-01,\n",
       "        -7.8519e-01, -1.2171e+00,  5.4993e-01,  5.9450e-01,  4.9948e-01,\n",
       "         3.2081e-01, -2.9923e-01,  1.1101e+00, -5.5765e-01,  1.8880e-01,\n",
       "        -3.5466e-02,  5.4179e-01,  3.4119e-01, -6.5781e-02, -1.4377e+00,\n",
       "        -7.4751e-01,  1.3443e+00,  9.7267e-01,  4.5958e-01, -2.2742e-01,\n",
       "         2.5788e-01,  2.3699e-01,  3.2163e-01,  6.1441e-02, -7.7064e-01,\n",
       "        -2.3956e-01,  6.8663e-01,  4.4078e-02, -7.2217e-01, -4.3073e-01,\n",
       "        -1.9779e-01, -8.3735e-01,  1.2509e+00,  5.6305e-01, -1.1801e+00,\n",
       "         2.0632e-01, -3.0702e-01, -2.0944e+00,  2.6588e-01,  2.2024e-01,\n",
       "        -1.5097e+00, -3.6570e-01,  1.4120e+00, -7.1473e-01, -3.1157e-01,\n",
       "        -4.6349e-01, -2.3427e-02, -7.0861e-01, -9.4280e-01, -7.7121e-01,\n",
       "         3.6975e-01, -8.4478e-02,  6.3305e-01,  1.3621e+00,  4.3800e-01,\n",
       "        -1.9275e-01, -3.1926e-01, -4.1457e-01, -8.9066e-01,  9.2342e-01,\n",
       "        -3.0146e-01, -2.1555e-01,  3.8328e-01,  5.1713e-01,  3.5908e-01,\n",
       "         1.1656e+00,  7.6597e-01, -3.4403e-01, -5.3937e-01,  4.9268e-01,\n",
       "         8.2114e-01, -8.1602e-01,  7.8236e-01,  3.0376e-01, -2.9936e-01,\n",
       "         2.5628e-01, -1.5582e-01, -4.4226e-01,  7.5725e-01, -3.1761e-01,\n",
       "        -1.6163e-01,  3.0045e-01,  3.0176e-02, -3.7387e-01, -5.8798e-01,\n",
       "        -4.9751e-01, -7.4485e-01,  2.5237e-01, -5.7931e-01, -5.6277e-01,\n",
       "         4.6521e-01,  3.6742e-01,  4.1979e-01, -1.5886e-01, -7.1090e-02,\n",
       "         1.6348e-01,  7.2769e-01,  4.9482e-01,  2.9852e-02,  3.0140e-01,\n",
       "        -1.4689e-01,  8.5570e-01,  9.2581e-02,  2.3287e-01,  6.1485e-01,\n",
       "        -9.3425e-02, -1.5787e-01,  3.7026e-02,  1.2235e+00, -3.5488e-01,\n",
       "        -4.2800e-01,  4.1157e-01,  3.7055e-01, -1.9062e-01,  1.4748e-01,\n",
       "         2.4523e-01,  4.6213e-01,  1.6799e-01, -8.1639e-01,  9.5775e-01,\n",
       "         2.0315e-01,  1.3364e+00,  1.0240e+00,  6.7771e-01, -2.5149e-01,\n",
       "        -5.5052e-01, -6.5725e-01, -2.9410e-01,  1.1996e+01,  2.3719e-01,\n",
       "         3.0850e-01,  9.8168e-01, -4.5634e-01, -3.4024e-01,  6.9245e-01,\n",
       "         5.1312e-01, -2.0268e-01,  1.2558e-01, -3.9725e-01, -5.6129e-01,\n",
       "        -1.3124e+00, -4.2881e-01, -4.2417e-01, -9.4222e-01,  6.4273e-01,\n",
       "         2.7553e-01,  3.7940e-01, -4.6368e-01, -1.0838e+00, -5.3508e-02,\n",
       "        -7.0658e-01,  2.8167e+00,  5.2866e-01, -1.1476e+00,  4.8749e-01,\n",
       "        -8.1228e-01,  9.8194e-01,  1.2713e+00, -2.7791e-01, -6.9846e-02,\n",
       "         3.1236e-03, -4.6690e-02,  1.2062e+00, -1.1297e+00,  9.5936e-01,\n",
       "         1.4393e+00,  7.4214e-01,  1.1655e-01, -9.3903e-02,  6.0218e-01,\n",
       "         5.0426e-03,  4.0291e-01, -2.7155e-01, -1.7259e-01,  4.4115e-01,\n",
       "         5.4150e-01,  1.1339e-01, -1.5378e-01,  5.3109e-01, -6.3452e-02,\n",
       "        -4.9398e-01, -9.6525e-02, -8.2315e-02, -8.2380e-02, -2.1047e-01,\n",
       "        -1.2777e+00, -1.6583e-01, -2.4563e-01,  5.0593e-02, -1.7202e-01,\n",
       "        -7.3104e-01,  3.4414e-02, -5.9132e-01, -5.7809e-01,  1.3961e+00,\n",
       "         1.2437e+00, -4.0123e-01,  4.5136e-01, -7.2891e-01,  1.2655e-01,\n",
       "        -3.7094e-01,  5.5349e-01,  8.7316e-01, -6.9604e-01,  3.1749e-01,\n",
       "        -3.5227e-01, -9.4339e-01,  1.3399e+00,  6.4968e-02,  4.4472e-02,\n",
       "         9.2050e-01, -4.8341e-01,  4.8583e-01,  7.3679e-02,  2.4777e-01,\n",
       "        -3.8371e-02, -6.9014e-01, -5.7413e-01, -9.3290e-01,  5.5911e-01,\n",
       "        -9.0531e-01,  2.3356e-01,  2.7213e-01,  7.2389e-01, -3.1946e-01,\n",
       "        -3.2858e-01,  3.6765e-01,  4.5807e-01, -3.9383e-01,  6.1640e-01,\n",
       "        -1.6622e-01, -1.6424e-02,  2.7199e-01,  3.5926e-01,  2.0439e-01,\n",
       "         8.5761e-01, -4.9925e-01,  1.4657e+00,  5.3387e-01,  5.8821e-01,\n",
       "         9.0209e-01,  6.9877e-01, -3.8226e-01, -4.9858e-01, -8.0371e-03,\n",
       "         5.6173e-01,  6.4445e-02,  2.2967e-02, -1.2741e+00,  1.2271e+00,\n",
       "        -5.0421e-01,  8.7067e-01, -5.2676e-02,  3.5395e-01, -1.5097e-02,\n",
       "        -1.8844e-01, -4.8687e-01, -9.2979e-01, -6.7770e-02,  9.7318e-01,\n",
       "        -2.8344e-01, -1.8178e-01,  8.0175e-01, -8.1949e-01,  4.1416e-01,\n",
       "         1.0091e-01, -2.2352e-01, -1.4045e-01,  4.6145e-01, -5.9100e-02,\n",
       "         2.6665e-02, -1.3137e+00, -2.2185e-01,  1.4398e-02,  1.1273e+00,\n",
       "        -2.1497e-01,  4.0204e-01, -6.7079e-01,  6.9423e-01, -1.0337e+00,\n",
       "         2.2024e-01, -7.0322e-02,  1.4341e-01,  3.8881e-01, -1.4936e-01,\n",
       "         4.2229e-01, -2.5882e-01, -3.2708e-01, -1.5587e-01, -3.1491e-02,\n",
       "        -5.2022e-01,  9.2258e-01, -4.0631e-01, -1.1413e+00,  4.7558e-01,\n",
       "        -2.5803e-01,  9.5926e-01, -8.4526e-01,  1.0862e+00, -1.5879e+00,\n",
       "        -3.2436e-01,  3.9342e-01, -7.1163e-01,  2.1924e-01,  4.3712e-01,\n",
       "         8.4183e-01,  1.8594e-01,  2.4150e-01])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_embedding(df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "bb069e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = df_train.iloc[10]['text']\n",
    "\n",
    "encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "\n",
    "token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "\n",
    "with torch.no_grad():\n",
    "     output = model_embed(**encoded)\n",
    "\n",
    "\n",
    "hidden_states = output['hidden_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e9935b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 31\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "211c4537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 31, 768])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "05795c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 31, 768])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ff153f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 13, 768])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "191e95d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 31 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "\n",
    "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "78ea8be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.9241e-01,  7.9641e-01, -2.0244e-01, -6.0815e-02,  9.8484e-02,\n",
       "         1.0083e+00,  1.7977e-01,  2.9316e-01, -2.3318e-01, -1.9373e-02,\n",
       "        -5.4172e-01,  1.1561e-01,  1.0236e+00, -4.7100e-01,  7.5856e-02,\n",
       "        -1.0722e+00,  4.7360e-01, -7.9421e-01,  6.4980e-01,  1.7483e+00,\n",
       "        -2.8437e-01, -1.2566e+00, -2.4930e-01, -2.5476e-01, -1.4871e+00,\n",
       "        -1.7104e-01, -2.3814e-01,  4.2275e-01,  4.2473e-01,  1.4769e+00,\n",
       "         3.7531e-01, -7.5099e-01,  5.7870e-01, -4.7627e-01, -2.6753e-01,\n",
       "         6.5057e-01,  1.9836e-02,  1.4142e-01,  2.6741e-01, -9.7723e-01,\n",
       "         1.0625e-01,  9.1154e-02,  1.0801e+00,  8.9258e-01,  8.3028e-01,\n",
       "        -5.8031e-01, -1.9542e-01,  1.3636e+00, -5.3820e-02, -7.7757e-01,\n",
       "        -4.9669e-02,  5.9412e-01,  4.6147e-01, -5.3151e-01,  1.4766e+00,\n",
       "        -3.6486e-01,  4.1003e-02, -3.5144e-01, -4.9082e-01,  5.1891e-01,\n",
       "        -1.4226e-01,  6.0131e-01,  1.0701e+00,  1.4234e-01,  6.9941e-01,\n",
       "         4.8953e-01, -7.3775e-01, -1.4916e+00,  4.0396e-01,  1.2616e+00,\n",
       "        -2.3029e-03, -2.4295e-01,  8.7709e-01, -1.1257e-01,  1.0054e-01,\n",
       "        -6.0755e-01,  1.9256e-01, -2.6108e+00, -5.9586e-01, -1.2154e-01,\n",
       "        -4.0851e-01, -4.5402e-01,  5.3489e-01, -7.6984e-02,  8.5235e-01,\n",
       "        -9.3999e-01,  3.8665e-01,  1.0739e+00, -1.1662e+00,  9.8238e-01,\n",
       "        -6.2845e-01, -2.7064e-01,  9.7704e-01,  3.1744e-01, -4.4525e-01,\n",
       "         5.3877e-01, -7.0805e-01,  4.4991e-01,  4.1260e-01, -4.1041e-01,\n",
       "         6.0829e-01, -1.0440e-02, -3.5600e-01, -3.0429e-01, -1.1542e+00,\n",
       "         1.8704e+00,  7.2829e-02, -2.0557e-01, -3.0802e-01, -2.5459e-01,\n",
       "        -4.6715e-01, -9.4575e-02, -7.8093e-02,  1.1684e+00, -1.2329e-01,\n",
       "        -1.1548e-01,  6.1125e-01,  4.0327e-01,  3.2356e-01,  3.0479e-01,\n",
       "        -2.5152e-01, -1.1312e+00,  2.2702e-01, -3.9472e-01, -3.3398e-01,\n",
       "         7.4902e-01,  6.4044e-01,  6.5890e-01, -1.0819e+00,  2.2245e-01,\n",
       "         2.2803e-01, -6.3193e-01,  5.6282e-01, -1.6275e+00, -1.4431e+00,\n",
       "        -7.1638e-01,  2.0741e-01,  9.3750e-01, -5.6409e-01, -7.1144e-01,\n",
       "        -3.6150e-02,  1.0280e-01,  3.8731e-01, -4.9296e-01, -2.9580e-01,\n",
       "        -6.5919e-01,  3.1638e-01,  3.5899e-01, -8.6584e-01, -8.2191e-02,\n",
       "        -8.1785e-02,  1.3324e+00,  4.4700e-02, -5.8956e-01, -8.3628e-01,\n",
       "         1.3747e+00,  5.0873e-01,  1.0921e+00,  2.7335e-01,  7.9550e-01,\n",
       "         1.2986e-01, -5.7004e-01, -7.4994e-01, -1.2382e+00,  5.9912e-02,\n",
       "         4.4660e-01,  5.0992e-02, -1.1233e-01, -2.9262e-01,  1.9261e-01,\n",
       "        -1.1032e+00, -1.2351e+00, -4.8586e-01,  1.6228e-02,  3.3735e-01,\n",
       "        -1.1197e+00,  1.1299e+00,  2.1134e-01, -1.1348e-01, -6.4488e-01,\n",
       "         2.1887e-01,  3.2991e-01,  1.0603e+00, -6.8222e-01,  1.0812e-01,\n",
       "         6.7838e-01, -7.6637e-02,  1.2439e+00,  1.2470e-01, -9.0949e-02,\n",
       "         1.2584e-01, -1.5596e-01,  1.0120e+00,  8.4141e-01, -4.3773e-01,\n",
       "        -1.8932e-01,  7.8314e-01, -7.2243e-01,  7.3893e-02, -1.7505e-01,\n",
       "        -1.0831e-01, -3.1618e-01, -2.3697e-02,  5.2847e-01,  7.6197e-01,\n",
       "        -3.3648e-01, -3.6877e-01, -4.7817e-01, -1.3363e-01,  1.0966e-01,\n",
       "         6.3174e-01,  3.0834e-01, -1.0350e+00,  6.9956e-01, -1.7498e-01,\n",
       "        -4.0540e-01,  3.3121e-02,  4.0679e-01, -7.9228e-01, -2.2247e-01,\n",
       "         9.3831e-01, -2.7159e-01,  4.3673e-01, -3.2890e-01, -4.1414e-01,\n",
       "         6.4539e-01,  1.3226e+00,  3.2060e-01,  1.0041e+00,  5.3574e-01,\n",
       "        -2.0082e-01,  2.8792e-01, -3.6286e-01, -1.3994e-01, -6.8664e-03,\n",
       "         9.3830e-01, -6.7324e-02,  2.9806e-01, -8.5330e-01, -2.0073e-01,\n",
       "        -8.4038e-01,  2.6138e-01, -5.6100e-01, -8.8183e-01, -1.1001e-01,\n",
       "         1.7131e+00, -9.9012e-01,  5.6881e-01,  4.2109e-01, -1.7130e-01,\n",
       "        -9.8227e-01,  1.1200e-02, -1.2243e-01, -6.3821e-01,  4.3629e-01,\n",
       "        -7.8857e-01, -8.2540e-01, -8.5849e-01, -1.0651e-02, -1.2927e+00,\n",
       "        -6.5670e-01, -3.8462e-01, -2.9848e-01,  1.4987e+00,  5.7918e-02,\n",
       "        -6.3851e-01, -7.3571e-01,  1.4591e-01, -8.8396e-01, -1.0409e+00,\n",
       "        -2.5874e-01,  8.1339e-01, -1.0737e+00, -2.3867e-01, -1.8173e-01,\n",
       "        -8.1545e-01,  1.2265e+00, -5.6605e-01, -7.7884e-01, -3.3391e-01,\n",
       "        -3.1387e-01,  1.1275e+00,  1.7039e+00, -6.3857e-01, -5.4720e-01,\n",
       "         2.3435e-01,  1.0323e+00, -7.6710e-04, -6.9414e-02, -9.0738e-02,\n",
       "        -8.9897e-01,  1.1417e-01, -4.0148e-01,  7.7040e-01,  5.9335e-01,\n",
       "         1.0887e+00, -5.5841e-02,  5.2613e-01,  4.1465e-01,  6.4371e-01,\n",
       "        -7.9994e-01,  2.7014e-01,  6.0098e-01,  6.6007e-01, -1.6392e-01,\n",
       "        -6.6728e-01, -2.0229e-01, -3.1064e-01,  9.6592e-01, -4.0449e-01,\n",
       "         2.1623e-02, -1.4066e-01,  1.6111e-01,  7.4431e-01, -2.7743e-01,\n",
       "         5.7770e-01,  3.9189e-01, -8.9664e-01, -9.6152e-02,  5.7757e-02,\n",
       "         5.7962e-01, -8.3397e-01, -3.3770e-01,  5.1246e-01, -6.2949e-01,\n",
       "        -9.7005e-01,  1.0498e+00,  1.2976e+00,  6.4733e-01, -1.8183e+00,\n",
       "        -1.3051e-01, -2.9316e-01, -9.4621e-01,  1.7914e-01,  3.2927e-01,\n",
       "         6.3452e-01,  3.9751e-01,  2.0274e-01,  1.0383e-03, -5.6990e-01,\n",
       "         9.8131e-01,  9.8243e-02,  5.6004e-02,  2.1863e-01, -1.6002e-01,\n",
       "        -6.1145e-01,  1.5280e-02,  3.9257e-01,  3.5027e-01, -6.5361e-01,\n",
       "         3.0320e-02,  8.9074e-02, -1.3191e+00, -1.1161e+00, -6.0786e-01,\n",
       "        -2.4217e+00,  6.2364e-01,  1.1441e+00,  3.4740e-01, -2.1923e-01,\n",
       "         4.1903e-01,  8.3162e-01, -3.0204e-01, -7.9938e-01, -7.7233e-01,\n",
       "        -1.0522e+00, -3.4739e-01,  2.9502e-01, -3.8976e-02, -6.0983e-01,\n",
       "         1.9762e-01, -6.6970e-02,  1.9480e-01, -2.7653e-01, -8.8610e-01,\n",
       "         2.2183e-01,  6.9371e-01, -3.2147e-01,  1.6580e+00, -5.5523e-01,\n",
       "         3.2115e-02,  1.6511e-01, -5.3489e-01,  1.1775e+00, -3.2722e-01,\n",
       "         8.1190e-02, -2.8774e-01, -2.4892e-02,  2.8745e-01, -7.9447e-01,\n",
       "        -1.3284e+00, -1.0025e+00,  2.2172e-02,  4.7558e-01,  4.6313e-01,\n",
       "         9.1291e-01,  7.0762e-01,  2.6650e-01,  9.8779e-02,  1.1249e+00,\n",
       "         1.1157e+00,  3.5173e-02, -2.9673e-01, -3.9675e-01, -8.0497e-01,\n",
       "        -3.9730e-01, -2.5362e-01,  2.8647e-01,  1.3565e+00,  1.6147e+00,\n",
       "         6.4709e-01,  1.4312e-01,  2.6593e-01, -8.9355e-01, -6.6851e-01,\n",
       "         6.0758e-01,  1.0502e+00, -1.4644e+00,  9.2982e-02, -1.2266e+00,\n",
       "        -8.6108e-01, -4.3615e-01,  1.9161e-01,  1.0330e+00,  8.3336e-01,\n",
       "        -4.4033e-01,  1.2102e+00, -1.0268e+00, -5.5147e-01, -4.0089e-01,\n",
       "        -2.2606e-01, -4.0155e-01,  8.4423e-01,  1.2973e-01,  1.5335e-01,\n",
       "        -6.7371e-01,  3.8535e-01,  4.9812e-01,  3.0213e-03,  7.3800e-01,\n",
       "        -3.2315e-01,  8.1191e-01,  1.3819e-01,  1.5358e-01,  4.7617e-01,\n",
       "         3.2186e-01,  3.1731e-01,  6.1113e-01, -1.1082e-01,  6.9823e-02,\n",
       "        -7.9593e-01,  1.3522e+00, -7.7755e-01, -1.7734e+00, -4.4784e-01,\n",
       "         1.3260e-01,  9.5389e-01,  6.7290e-01,  3.8669e-01, -4.9885e-01,\n",
       "         5.8615e-01,  1.3277e-01, -5.7696e-01,  9.3736e-01, -1.2060e+00,\n",
       "        -3.4364e-01,  5.5493e-01,  6.1506e-01,  2.9838e-01, -7.0746e-01,\n",
       "        -5.7933e-01,  1.1958e+00, -4.1729e-01,  1.6107e+00,  8.0619e-01,\n",
       "        -7.5276e-01, -8.4444e-01,  7.3971e-01,  4.2257e-01, -9.5569e-01,\n",
       "         3.1746e-01, -6.8546e-01,  1.7971e-01,  4.6877e-02,  4.8592e-01,\n",
       "         3.1640e-01,  5.6751e-01, -9.0961e-01, -1.2439e-01,  4.7496e-02,\n",
       "        -1.4039e-01,  3.9934e-01, -1.2531e+00,  1.9992e-01, -6.4533e-01,\n",
       "        -4.7911e-01, -8.1936e-01, -9.0813e-01, -4.2494e-01, -6.1400e-01,\n",
       "         7.2493e-01,  8.2301e-01, -1.3256e+00, -1.5536e+00, -2.3355e-01,\n",
       "        -3.4878e-01, -1.0008e+00,  3.6537e-01,  2.5502e-01, -2.2196e-01,\n",
       "         8.6199e-01,  8.8723e-01, -6.5896e-01, -1.1500e+00,  4.7111e-01,\n",
       "        -1.8757e-01,  8.6923e-02,  4.0952e-02, -9.6450e-01,  1.1842e+00,\n",
       "         4.1096e-01,  1.7495e+00, -9.9939e-01,  3.3065e-01, -9.5278e-01,\n",
       "        -5.3129e-01,  1.6427e-01, -8.9963e-01,  1.4796e-01,  1.0369e-02,\n",
       "         8.1918e-01,  8.7517e-01, -7.5180e-01,  1.0734e-01, -1.3859e-01,\n",
       "        -4.1002e-01,  8.0603e-01,  1.4095e+00,  4.9810e-01,  7.9325e-02,\n",
       "         3.0305e-01,  1.7137e-01,  3.1203e-01,  1.6266e-02, -1.5630e+00,\n",
       "         1.4787e+00,  8.8182e-02, -6.6748e-01,  1.4480e-01, -1.4620e-01,\n",
       "         7.0249e-01,  4.9983e-01, -5.5068e-02, -2.9791e-01, -2.3619e-02,\n",
       "        -4.4803e-01,  1.2457e-01, -4.2389e-01,  1.2172e+00, -4.4376e-01,\n",
       "         1.2697e+00,  4.5701e-01,  4.5511e-01,  4.0780e-01,  6.3825e-01,\n",
       "         8.1847e-01,  6.9038e-01, -8.0615e-01,  3.5566e-01,  1.0052e-01,\n",
       "         1.7535e+00, -2.0884e-01, -6.7054e-01,  7.5058e-01, -7.7362e-01,\n",
       "        -1.9423e-01,  7.8593e-01,  2.4009e-01,  8.3361e-01,  5.1954e-01,\n",
       "         1.1393e+00,  7.2397e-01, -5.4689e-01, -1.6353e-01,  5.3296e-01,\n",
       "         1.1954e+00, -6.6383e-01,  6.7425e-01,  1.1797e+01,  2.9316e-01,\n",
       "         4.6181e-01,  2.0841e-01,  1.6232e-01, -1.0482e+00, -3.6694e-01,\n",
       "        -2.2675e-01,  2.8078e-01,  1.1988e+00, -2.2342e-01,  1.2247e+00,\n",
       "         8.3522e-01,  8.1191e-01,  2.4421e-01, -1.3721e+00, -1.4754e-02,\n",
       "        -5.5667e-01, -1.2093e-01, -5.1870e-02,  8.3113e-01, -1.8108e-01,\n",
       "         6.8265e-01,  1.6293e+00,  1.2195e+00, -6.1087e-01,  1.6203e-01,\n",
       "        -5.4741e-01, -1.7563e+00, -5.1416e-02, -6.9662e-01,  1.0534e+00,\n",
       "        -1.3218e+00, -3.0476e-01, -2.1309e-01, -6.1706e-01, -1.0412e+00,\n",
       "        -5.4111e-01, -5.3510e-01, -1.4031e-01, -1.5246e+00,  5.3545e-01,\n",
       "        -1.9919e-01, -2.9239e-01, -5.5541e-01,  8.5105e-01,  4.0783e-01,\n",
       "         1.9646e-01, -1.0820e+00, -5.6298e-01,  7.9669e-02,  2.9089e-01,\n",
       "        -3.9488e-01, -1.1514e-01, -7.1660e-01, -4.4592e-01, -1.3524e+00,\n",
       "        -7.0875e-02,  1.1788e+00,  1.2016e-01,  7.3410e-01,  2.9099e-01,\n",
       "         8.5904e-01,  6.2286e-01,  1.5034e-01,  2.7569e-01,  7.7637e-01,\n",
       "         2.4620e-01, -4.4633e-01,  1.3731e+00,  2.7523e-01,  8.5573e-01,\n",
       "         3.3839e-01,  3.9782e-01,  1.3723e+00, -2.5743e-01, -1.8757e-01,\n",
       "         3.6681e-01, -1.2171e-02, -2.1544e-01,  8.8569e-01, -1.9932e-01,\n",
       "        -7.7676e-01, -2.7487e-01,  3.9534e-02,  1.0789e+00, -1.1149e-02,\n",
       "         9.3493e-02, -2.0077e+00, -1.1138e+00,  9.2222e-01,  1.7612e-01,\n",
       "         1.5251e+00,  2.0129e-01,  8.3959e-01,  1.0115e+00, -2.4440e-02,\n",
       "        -1.0475e+00,  7.7050e-01,  8.6710e-02,  4.7257e-01, -2.5391e-01,\n",
       "        -4.1817e-01,  1.9184e-02,  5.4867e-01, -3.5882e-01, -4.3600e-01,\n",
       "        -7.1741e-01, -1.0753e+00,  2.2833e-02, -2.8358e-01, -1.1461e-01,\n",
       "        -3.9783e-01,  3.4415e-01,  1.4521e-01,  2.6118e-01,  5.3179e-01,\n",
       "        -4.9045e-01,  1.0715e+00,  4.5974e-01, -2.6159e-01,  3.2466e-01,\n",
       "         2.0928e-01, -6.0495e-01,  3.5261e-01, -2.3358e-01,  2.6399e-01,\n",
       "        -7.9725e-01, -3.0696e-01, -1.3583e+00, -5.8321e-01,  6.2422e-01,\n",
       "        -6.6334e-01,  2.8539e-01, -3.3681e-01, -6.7382e-01, -5.8588e-01,\n",
       "         5.0481e-01,  5.9713e-01,  1.0331e+00, -5.9626e-01,  1.2278e+00,\n",
       "        -1.0190e+00, -1.3638e+00, -8.8828e-01,  2.8062e-01, -3.0747e-01,\n",
       "         9.7416e-04, -9.3722e-02, -2.4310e-02,  3.5910e-01,  3.9832e-01,\n",
       "        -2.7156e-02,  6.0073e-01,  2.4154e-01,  1.0270e+00, -1.6606e-01,\n",
       "        -1.0137e+00,  1.0309e+00, -2.6445e-02,  5.5315e-01, -2.1356e-01,\n",
       "        -5.2641e-01,  1.6687e-01,  6.9332e-01, -4.4600e-01,  1.0141e-01,\n",
       "        -7.7866e-01, -5.0939e-01, -4.5028e-01, -1.2503e-01,  2.9829e-01,\n",
       "         5.3308e-01,  5.7999e-01, -2.6368e-01,  9.6866e-02, -2.0941e-01,\n",
       "         3.7842e-01,  2.0834e-01,  3.7624e-02])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2acdfa24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.4470e-01,  7.1697e-01,  2.5688e-02, -1.6738e-01,  2.3583e-01,\n",
       "         6.6787e-01, -1.3944e-01,  1.3767e-01, -1.3260e-01, -1.4556e-01,\n",
       "        -3.5052e-01,  4.7551e-01,  7.1150e-01, -1.2456e-01,  2.3886e-01,\n",
       "        -1.2112e+00,  2.1870e-01, -6.2340e-01,  7.4428e-01,  1.7221e+00,\n",
       "        -3.4774e-01, -9.0149e-01, -3.3343e-01, -4.8426e-01, -1.1375e+00,\n",
       "        -1.6021e-01, -4.5457e-01,  6.6641e-01,  3.0233e-01,  1.5861e+00,\n",
       "         5.3190e-01, -3.3844e-01,  3.1331e-01, -6.6308e-01, -2.0351e-01,\n",
       "         8.9602e-01, -1.7156e-01,  1.5755e-01,  6.3668e-01, -8.1655e-01,\n",
       "         5.4786e-01,  2.0781e-02,  1.3678e+00,  7.6663e-01,  8.0346e-01,\n",
       "        -1.3833e-01, -3.5393e-02,  1.5194e+00, -1.7736e-01, -5.7622e-01,\n",
       "        -1.9066e-01,  5.5848e-01,  2.2112e-01, -9.2591e-01,  1.0626e+00,\n",
       "        -5.1608e-01,  8.2437e-02, -4.1658e-01, -3.2709e-01,  5.6486e-01,\n",
       "        -1.0597e-01,  5.9780e-01,  6.6816e-01, -1.4532e-01,  6.0984e-01,\n",
       "         6.2301e-01, -9.8247e-01, -1.9989e+00,  2.6051e-01,  1.0460e+00,\n",
       "        -2.2550e-01, -5.2914e-01,  1.3581e+00,  3.4914e-01, -8.5189e-02,\n",
       "        -2.4926e-01,  7.7867e-02, -2.1635e+00, -3.8946e-01, -3.1105e-02,\n",
       "        -6.9488e-01, -5.5145e-01,  8.5743e-01,  3.9458e-02,  4.6777e-01,\n",
       "        -9.4950e-01,  7.1284e-01,  8.7303e-01, -1.0583e+00,  1.2589e+00,\n",
       "        -7.0621e-01, -2.7646e-01,  1.0433e+00,  9.9430e-01,  8.9915e-02,\n",
       "         7.4356e-01, -6.0162e-01,  8.6613e-01,  2.4141e-01,  1.6481e-02,\n",
       "         7.5947e-01,  6.2322e-01, -3.2145e-01,  7.8812e-02, -1.2446e+00,\n",
       "         1.5429e+00, -5.2413e-02, -4.4520e-01, -4.7952e-01, -2.6765e-01,\n",
       "        -6.5692e-01,  1.5530e-01,  1.3560e-01,  1.3973e+00,  3.2593e-01,\n",
       "         6.6276e-02,  5.6804e-01,  4.6006e-01,  2.2688e-01,  2.3199e-01,\n",
       "        -3.1114e-01, -9.5073e-01,  8.6046e-02, -3.5868e-01, -5.9798e-01,\n",
       "         5.6769e-01,  2.3300e-01,  2.3846e-01, -1.1735e+00,  2.2625e-01,\n",
       "        -2.3454e-01, -1.3269e-01,  1.0108e+00, -1.1302e+00, -1.1913e+00,\n",
       "        -1.0434e+00,  5.2745e-01,  1.0548e+00, -7.3563e-01, -5.7254e-01,\n",
       "        -7.0563e-02,  4.6988e-01,  1.0330e+00, -5.7432e-01, -6.9987e-01,\n",
       "        -5.3183e-01,  6.7946e-01,  4.1135e-01, -9.9363e-01, -1.3087e-01,\n",
       "        -3.4527e-01,  1.6396e+00,  2.5397e-01, -1.0335e+00, -6.4149e-01,\n",
       "         1.5679e+00,  5.3861e-01,  7.2424e-01,  3.0801e-01,  3.5677e-01,\n",
       "         2.9617e-01, -9.3625e-01, -2.8678e-01, -1.1097e+00, -5.2120e-02,\n",
       "         1.4362e-01,  9.8165e-02, -5.8560e-01,  7.9136e-01,  4.5453e-01,\n",
       "        -7.9574e-01, -1.1340e+00, -3.1522e-01, -7.3100e-02, -2.8710e-02,\n",
       "        -1.0751e+00,  1.0127e+00, -1.2373e-01,  1.7831e-01, -8.5020e-01,\n",
       "        -1.9391e-02,  1.8774e-02,  9.5470e-01, -7.6237e-01,  4.1939e-01,\n",
       "         4.4720e-01, -1.9623e-01,  1.3887e+00, -4.3271e-01, -1.6968e-02,\n",
       "        -1.3048e-01, -4.7960e-01,  6.6496e-01,  7.0257e-01, -7.4246e-01,\n",
       "        -6.9267e-02,  7.6261e-01, -3.8688e-01,  4.5496e-01, -1.4214e-02,\n",
       "        -1.5329e-01, -5.5821e-01,  2.4091e-01,  1.3586e-01,  8.2553e-01,\n",
       "        -1.8634e-01, -1.9795e-01, -4.5566e-01,  7.8622e-03,  1.7778e-01,\n",
       "         5.7483e-01,  3.6987e-01, -7.6150e-01,  7.6335e-01,  8.8621e-02,\n",
       "        -8.7341e-01,  7.2947e-02,  4.6469e-01, -6.4547e-01, -3.8627e-01,\n",
       "         1.2959e+00, -4.2062e-01,  4.7506e-01, -1.5574e-01, -1.4735e-01,\n",
       "         4.8303e-01,  4.4094e-01,  5.6709e-02,  5.8228e-01,  2.6665e-01,\n",
       "        -3.0908e-01,  4.5559e-01, -1.4050e-03, -3.9957e-01,  9.5517e-02,\n",
       "         7.0354e-01, -2.5548e-01,  3.0571e-01, -9.0614e-01, -1.0966e-01,\n",
       "        -7.6941e-01,  3.1169e-01, -6.1485e-01, -3.0931e-01, -4.6583e-01,\n",
       "         1.8959e+00, -7.7993e-01,  9.5848e-01,  2.2432e-01, -2.3965e-01,\n",
       "        -1.2092e+00,  2.3219e-01, -6.7015e-02, -7.0866e-01,  6.0838e-01,\n",
       "        -4.6104e-01, -9.7619e-01, -6.2548e-01, -1.1030e-01, -1.3134e+00,\n",
       "        -1.1759e+00, -3.4528e-01, -1.6799e-01,  1.6418e+00, -1.3379e-01,\n",
       "        -5.7817e-01, -1.2159e+00,  3.3516e-01, -4.5351e-01, -5.2955e-01,\n",
       "        -3.1792e-01,  9.1738e-01, -1.0426e+00, -1.8852e-01,  1.6509e-01,\n",
       "        -1.1488e+00,  1.8562e+00, -7.7982e-01, -6.4850e-01, -5.8612e-01,\n",
       "        -1.9729e-01,  1.1058e+00,  1.1476e+00, -2.3813e-01, -5.7662e-01,\n",
       "         3.2329e-01,  1.1752e+00,  1.2148e-01,  5.0276e-01,  6.9230e-03,\n",
       "        -6.9565e-01, -1.2408e-01, -1.6049e-01,  2.7949e-01,  3.9210e-01,\n",
       "         1.4168e+00,  5.9987e-02,  5.9164e-01,  1.8366e-02,  5.6025e-01,\n",
       "        -5.7639e-01,  3.4791e-02,  6.8212e-01,  4.4257e-01, -4.5215e-01,\n",
       "        -9.8104e-01, -1.6812e-01, -4.1411e-01,  9.0074e-01, -7.3297e-01,\n",
       "         4.8435e-02, -1.4160e-01,  8.4938e-01,  5.1041e-01, -4.2098e-01,\n",
       "         5.1966e-01,  7.1357e-01, -5.6819e-01,  5.2533e-02, -6.8437e-03,\n",
       "         1.0678e+00, -1.0611e+00,  8.4847e-02,  4.1567e-01, -5.3849e-01,\n",
       "        -7.3597e-01,  9.9064e-01,  1.3020e+00,  6.1906e-01, -1.5088e+00,\n",
       "         1.4649e-01, -1.4054e-01, -5.2352e-01,  2.8713e-01,  7.5976e-02,\n",
       "         7.8112e-01,  3.1135e-01,  1.4871e-01,  2.7240e-01, -1.2285e-01,\n",
       "         1.4634e+00,  4.5965e-01,  1.3319e-01, -1.5366e-01, -3.7025e-02,\n",
       "        -1.0706e+00,  3.0904e-01,  2.1252e-02,  3.1591e-01, -3.8635e-01,\n",
       "         2.4281e-01,  7.4366e-01, -1.0658e+00, -1.2787e+00, -5.5949e-01,\n",
       "        -2.3918e+00,  8.1554e-01,  5.5513e-01,  2.3264e-01, -2.3683e-01,\n",
       "         4.3844e-01,  5.1208e-01, -3.2607e-01, -4.4740e-01, -8.8230e-01,\n",
       "        -1.0881e+00, -7.3561e-01,  4.2142e-01, -3.9715e-01, -3.1772e-01,\n",
       "        -7.3663e-04, -6.6527e-02,  9.7761e-02, -2.0951e-01, -7.9926e-01,\n",
       "         3.0210e-01,  8.2058e-01, -4.0680e-01,  1.4698e+00, -4.6543e-01,\n",
       "        -5.5226e-02,  1.0358e-01, -5.8945e-01,  1.0421e+00,  4.9309e-02,\n",
       "         3.5228e-01, -3.4276e-01, -5.2913e-01,  5.8739e-01, -9.8157e-01,\n",
       "        -8.1757e-01, -6.9580e-01, -1.0167e-01,  3.9725e-01,  7.0344e-01,\n",
       "         4.1843e-01,  6.9395e-01,  3.9221e-01, -1.2176e-01,  8.3349e-01,\n",
       "         1.0861e+00, -6.2377e-02, -3.1828e-01, -3.8884e-01,  1.7389e-01,\n",
       "        -4.6952e-01, -3.4415e-01, -6.1447e-03,  1.2511e+00,  1.3888e+00,\n",
       "         7.0038e-01,  5.5982e-01, -1.3507e-01, -8.3032e-01, -2.0171e-01,\n",
       "         2.5288e-01,  1.2732e+00, -1.5742e+00,  2.3590e-01, -8.6731e-01,\n",
       "        -4.7731e-01, -7.7027e-01,  3.4506e-01,  1.2014e+00,  6.9397e-01,\n",
       "        -7.6516e-01,  1.4024e+00, -7.9044e-01, -9.0320e-01, -2.0549e-01,\n",
       "         2.1280e-01, -8.4237e-01,  4.2444e-01, -2.8184e-01,  2.9444e-01,\n",
       "        -8.9480e-01,  7.5108e-02,  3.1847e-01,  3.1679e-01,  6.1304e-01,\n",
       "        -5.9144e-02,  7.8180e-01,  2.1244e-01, -6.2987e-02,  3.0298e-01,\n",
       "         1.7018e-01,  8.2738e-01,  5.2355e-01,  1.3368e-01, -3.7909e-01,\n",
       "        -6.6338e-01,  9.2982e-01, -9.1269e-01, -1.1530e+00, -1.9942e-01,\n",
       "         2.8758e-01,  5.1788e-01,  3.3534e-01,  5.8087e-01, -1.7039e-01,\n",
       "         6.5571e-01,  1.8708e-01,  8.6881e-03,  8.5786e-01, -1.6673e+00,\n",
       "        -2.3694e-01,  3.5381e-01,  4.8207e-01,  7.9793e-01, -8.8336e-01,\n",
       "        -3.3826e-01,  1.0524e+00, -4.5105e-01,  1.2290e+00,  7.7734e-01,\n",
       "        -2.2915e-01, -9.7111e-01,  4.4294e-01,  3.2794e-02, -1.1107e+00,\n",
       "         2.8430e-01, -9.6114e-01,  3.3873e-02,  4.8253e-02,  3.9225e-01,\n",
       "         4.6171e-01,  3.5571e-01, -8.4980e-01,  1.2688e-01,  2.4587e-01,\n",
       "         1.0621e-01,  6.3154e-01, -9.9148e-01, -1.0849e-01, -3.0105e-01,\n",
       "        -5.2966e-01, -4.5799e-01, -4.6841e-01,  7.5559e-02, -4.4059e-01,\n",
       "         7.9814e-01,  8.0692e-01, -8.9318e-01, -1.6544e+00, -6.7654e-01,\n",
       "        -5.9738e-01, -7.6605e-01,  1.2149e-01,  2.0838e-01,  6.3330e-01,\n",
       "         7.3249e-01,  9.1779e-01, -6.4624e-01, -1.1234e+00, -3.4394e-01,\n",
       "        -1.5665e-01,  2.7657e-01,  1.3004e-01, -1.2609e+00,  1.1166e+00,\n",
       "        -5.6477e-03,  1.5472e+00, -7.7052e-01,  3.3355e-01, -7.2022e-01,\n",
       "        -6.4893e-01, -1.8666e-01, -1.0660e+00,  2.9612e-01, -1.3879e-01,\n",
       "         4.6447e-01,  7.9415e-01, -4.8238e-01, -4.2177e-01, -6.3043e-01,\n",
       "        -7.3098e-01,  3.9182e-01,  1.3680e+00,  4.5244e-01,  3.5545e-01,\n",
       "         3.2200e-01,  2.2629e-01,  3.7585e-03,  2.6801e-01, -1.7323e+00,\n",
       "         1.1489e+00,  1.6692e-01, -4.3142e-01, -2.0523e-01, -2.8903e-01,\n",
       "         6.1501e-01,  2.2392e-01, -2.6357e-01, -2.6654e-01,  1.0148e-01,\n",
       "        -1.3371e-01,  1.7498e-01, -5.9340e-01,  8.9452e-01, -6.1787e-01,\n",
       "         1.3547e+00,  4.7522e-01,  2.1671e-01,  2.6787e-01,  6.1055e-01,\n",
       "         9.6912e-01,  3.2091e-01, -6.5262e-01,  2.7332e-01,  3.1888e-01,\n",
       "         1.0961e+00, -4.5652e-01, -7.0681e-01,  5.0080e-01, -6.7961e-01,\n",
       "        -6.4831e-01,  1.0208e+00,  2.3124e-01,  1.2255e+00,  4.7512e-01,\n",
       "         1.1953e+00,  4.1447e-01, -4.0365e-01, -2.2119e-01,  6.4097e-01,\n",
       "         1.2908e+00,  1.0296e-01,  8.1197e-01,  1.1400e+01,  4.1851e-01,\n",
       "         3.4773e-01, -1.5105e-01,  3.4307e-01, -1.2310e+00, -6.0246e-01,\n",
       "        -5.0314e-01,  7.0329e-02,  1.1583e+00, -4.7127e-01,  1.3094e+00,\n",
       "         9.7942e-01,  7.5103e-01,  6.2747e-01, -1.1628e+00, -6.7859e-02,\n",
       "        -4.4246e-01, -3.9051e-01, -2.6495e-01,  1.2218e+00, -3.6240e-02,\n",
       "         5.5817e-01,  1.5379e+00,  1.2835e+00, -8.4900e-01, -1.3131e-01,\n",
       "        -7.6401e-01, -1.7565e+00, -2.8776e-01, -1.1233e+00,  1.0676e+00,\n",
       "        -1.1774e+00, -8.8794e-01, -3.6251e-01, -8.9329e-01, -1.2964e+00,\n",
       "        -4.8513e-01, -4.1417e-01, -3.0957e-01, -1.7615e+00,  3.0447e-01,\n",
       "        -1.1067e-01, -2.3969e-01, -2.5133e-01,  7.7350e-01,  3.8805e-01,\n",
       "         3.0759e-01, -8.2779e-01, -8.9774e-01, -1.3678e-02,  3.4294e-01,\n",
       "        -3.1989e-03, -4.0818e-01, -2.8400e-01, -7.1747e-02, -1.5367e+00,\n",
       "        -6.3999e-02,  8.6262e-01,  1.9194e-01,  4.1291e-01,  7.9094e-02,\n",
       "         1.2196e+00,  8.3429e-01,  5.0151e-01,  3.6674e-01,  8.2252e-01,\n",
       "         6.4089e-01, -6.3189e-01,  1.4544e+00,  7.1091e-01,  6.0115e-01,\n",
       "         3.2881e-01,  5.4277e-01,  1.1146e+00, -2.9786e-01, -3.6983e-01,\n",
       "         4.0765e-01,  3.3973e-02, -2.6235e-02,  7.6283e-01, -2.1140e-01,\n",
       "        -1.0269e+00,  1.4660e-01,  1.9558e-01,  8.0457e-01,  1.3979e-01,\n",
       "        -3.3526e-01, -1.9780e+00, -1.2694e+00,  9.7955e-01,  3.7902e-02,\n",
       "         1.2098e+00,  2.6538e-01,  9.2663e-01,  1.2586e+00,  2.1673e-01,\n",
       "        -7.7369e-01,  5.7228e-01, -3.1810e-01,  4.0792e-01, -3.3391e-01,\n",
       "        -5.5221e-01,  3.3413e-01,  6.9611e-01, -2.8350e-01, -5.2451e-01,\n",
       "        -3.8431e-01, -9.0916e-01, -2.2672e-01,  2.1286e-02,  4.8826e-02,\n",
       "        -5.1710e-01,  8.5961e-02, -4.1159e-02,  2.6744e-01,  4.3153e-01,\n",
       "        -3.9215e-01,  1.0077e+00,  1.7562e-01, -4.8529e-01,  5.2522e-01,\n",
       "         4.9715e-01, -6.0199e-01,  9.5027e-03, -3.9767e-02, -2.1113e-02,\n",
       "        -1.2816e+00, -2.8144e-01, -9.9566e-01, -7.6027e-01,  5.6081e-01,\n",
       "        -5.6675e-01, -2.9091e-02, -4.4976e-01, -3.9944e-01, -1.0580e+00,\n",
       "         5.8993e-01,  3.9487e-01,  8.7007e-01, -4.7788e-01,  1.1376e+00,\n",
       "        -7.6825e-01, -9.4219e-01, -6.8711e-01,  8.5603e-01, -2.8396e-01,\n",
       "        -9.2883e-03, -5.0119e-02, -2.9703e-01,  1.9299e-01,  4.6784e-01,\n",
       "         3.0638e-01,  7.1259e-01,  1.9890e-01,  6.3688e-01, -6.3116e-02,\n",
       "        -8.1491e-01,  6.3606e-01, -2.0239e-01,  2.9730e-01, -8.2713e-02,\n",
       "        -5.2771e-01, -2.4907e-01,  8.2947e-01,  1.5031e-01,  2.4002e-03,\n",
       "        -7.5022e-01, -4.7066e-01, -4.4747e-01, -4.4307e-01,  1.9864e-01,\n",
       "        -6.9724e-03,  8.6137e-01, -7.0531e-02,  3.7697e-01, -6.4473e-02,\n",
       "         5.2002e-01,  1.7915e-01, -1.7646e-01])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87149999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0fb16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "bf6f1c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6246,  1.2481,  5.7773, -1.0492, -1.8568, -2.8500, -2.5932]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9f37ebd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['hidden_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "309271e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.6518e-01, -4.8222e-02,  1.4864e-04,  ..., -7.6252e-02,\n",
       "            8.4043e-02,  1.3102e-02],\n",
       "          [-6.0285e-02, -3.1149e-01,  1.5629e-01,  ...,  1.0649e-02,\n",
       "            1.2823e-01,  2.6586e-01],\n",
       "          [ 1.5781e-01, -4.8849e-03, -3.7191e-02,  ...,  2.6678e-01,\n",
       "           -3.5979e-01,  4.6399e-02],\n",
       "          ...,\n",
       "          [ 9.7358e-01, -6.6068e-01, -3.9619e-01,  ...,  2.9260e-01,\n",
       "            1.6849e-01,  3.7391e-01],\n",
       "          [ 5.4959e-01, -2.7490e-01,  1.7637e-01,  ..., -1.1539e-01,\n",
       "            2.9791e-02,  2.7837e-01],\n",
       "          [ 1.6472e-01, -1.5221e-01,  5.0608e-02,  ...,  3.8052e-01,\n",
       "            1.5168e-01,  1.2706e-01]]]),\n",
       " tensor([[[ 1.3622e-02,  6.2574e-02, -9.9679e-03,  ..., -8.2492e-03,\n",
       "            8.5152e-04, -8.5083e-02],\n",
       "          [-2.2514e-01, -7.8329e-01,  2.2409e-01,  ..., -6.1050e-01,\n",
       "            9.5662e-02,  7.0830e-01],\n",
       "          [-7.3169e-02, -4.8602e-02, -1.2962e-02,  ...,  1.3357e-01,\n",
       "           -5.8045e-01,  5.1867e-01],\n",
       "          ...,\n",
       "          [ 5.7936e-01, -9.3797e-01, -6.9967e-01,  ...,  3.1853e-01,\n",
       "            3.7234e-01,  5.4348e-01],\n",
       "          [ 7.3354e-01, -8.3514e-01,  4.1049e-01,  ..., -8.0608e-02,\n",
       "           -1.1184e-01,  9.0793e-01],\n",
       "          [-2.0765e-01, -3.4595e-01,  3.4737e-01,  ...,  7.4349e-01,\n",
       "           -2.0097e-01, -3.0694e-01]]]),\n",
       " tensor([[[ 0.0635,  0.0251, -0.0469,  ...,  0.0022,  0.0113,  0.0066],\n",
       "          [ 0.3267, -1.1685,  0.6190,  ..., -0.6610,  0.2516,  0.3670],\n",
       "          [ 0.4279,  0.1384,  0.3686,  ...,  0.0348, -0.8816,  0.5955],\n",
       "          ...,\n",
       "          [ 0.5613, -1.3369, -0.8684,  ...,  0.1989,  0.4018,  0.2461],\n",
       "          [ 0.5104, -0.5027,  0.2664,  ..., -0.3239, -0.2072,  0.9012],\n",
       "          [-0.4090,  0.1375,  0.2283,  ...,  0.9493,  0.2799, -0.4396]]]),\n",
       " tensor([[[ 0.0164,  0.0546,  0.0125,  ...,  0.0239,  0.0365, -0.0208],\n",
       "          [ 0.3269, -0.6613,  0.4602,  ..., -0.4257,  0.5575,  0.2987],\n",
       "          [ 0.2177,  0.4360,  0.6614,  ...,  0.1371, -0.4378,  0.1186],\n",
       "          ...,\n",
       "          [ 0.4842, -0.8237, -0.2558,  ..., -0.0593,  0.6798, -0.0670],\n",
       "          [ 0.1652, -0.3003,  0.5891,  ..., -0.3871,  0.1684,  0.5456],\n",
       "          [-0.0134,  0.3164,  0.3719,  ...,  0.5641,  0.3138, -0.1638]]]),\n",
       " tensor([[[ 0.0157,  0.0906, -0.0375,  ...,  0.0505, -0.0322,  0.0328],\n",
       "          [ 0.2557, -0.4936,  0.2843,  ..., -0.4030,  0.6520,  0.2079],\n",
       "          [ 0.2844,  0.0973,  0.0823,  ...,  0.1512, -0.6047,  0.4141],\n",
       "          ...,\n",
       "          [-0.0293, -0.8482, -0.3378,  ..., -0.1608,  0.2653, -0.3355],\n",
       "          [ 0.0749, -0.3808,  0.1914,  ..., -0.5887,  0.1733,  0.7172],\n",
       "          [ 0.1330,  0.1628,  0.1271,  ...,  0.0801,  0.1575, -0.0421]]]),\n",
       " tensor([[[-0.0626, -0.0726,  0.1174,  ..., -0.0918,  0.0235,  0.1078],\n",
       "          [ 0.3850, -0.4702,  0.1666,  ..., -0.5401,  0.6730,  0.0883],\n",
       "          [ 0.3624,  0.1089, -0.4043,  ..., -0.2895,  0.1725, -0.2437],\n",
       "          ...,\n",
       "          [ 0.0160, -0.9901, -0.7257,  ..., -0.1599, -0.2142, -0.3327],\n",
       "          [-0.3247, -0.2190,  0.2347,  ..., -0.8718,  0.8852,  0.5989],\n",
       "          [-0.0163,  0.0248,  0.0450,  ...,  0.0920, -0.0202,  0.0096]]]),\n",
       " tensor([[[-3.2076e-02, -1.4610e-02,  1.1710e-01,  ..., -1.2963e-01,\n",
       "            2.7423e-02, -3.8412e-02],\n",
       "          [ 6.6266e-01, -1.1507e+00,  9.4588e-02,  ..., -6.9694e-01,\n",
       "            7.9932e-01, -3.6453e-01],\n",
       "          [-2.9038e-02, -4.2825e-01,  1.1749e-01,  ...,  4.4092e-02,\n",
       "            8.5202e-02, -2.0015e-01],\n",
       "          ...,\n",
       "          [ 3.9687e-01, -9.0846e-01, -9.3333e-01,  ..., -4.3328e-01,\n",
       "           -5.1867e-01, -4.1843e-01],\n",
       "          [-3.5050e-01, -1.0035e+00, -4.7167e-02,  ..., -1.1417e+00,\n",
       "            1.3556e+00,  6.7690e-01],\n",
       "          [-4.3019e-03, -2.6045e-04,  8.0340e-02,  ..., -1.9579e-02,\n",
       "           -7.4736e-02, -5.5165e-03]]]),\n",
       " tensor([[[-0.0552, -0.0472, -0.1967,  ...,  0.0252,  0.1202, -0.0055],\n",
       "          [ 0.5079, -1.0346,  0.1572,  ..., -1.1626,  0.8291, -0.3168],\n",
       "          [-0.5602, -0.6438,  0.2432,  ..., -0.7026,  0.7819, -0.3188],\n",
       "          ...,\n",
       "          [ 0.0932, -0.7089, -0.4219,  ..., -0.8654,  0.0870, -0.7770],\n",
       "          [-0.4521, -1.4466, -0.1787,  ..., -2.0520,  1.4534,  0.4213],\n",
       "          [-0.0283,  0.0104,  0.0361,  ...,  0.0266,  0.0464,  0.0100]]]),\n",
       " tensor([[[-0.4523, -0.2057,  0.1063,  ...,  0.1170, -0.1822,  0.0327],\n",
       "          [ 1.0276, -0.7412, -0.0865,  ..., -0.5873,  0.8936, -1.8135],\n",
       "          [ 0.1905, -1.0908,  0.4512,  ..., -0.5834,  0.4579, -1.2885],\n",
       "          ...,\n",
       "          [ 0.3493, -0.5657, -0.5159,  ..., -0.5958,  0.4234, -1.0273],\n",
       "          [ 0.7542, -1.4590,  0.1595,  ..., -1.7797,  0.8575, -0.3664],\n",
       "          [ 0.0295,  0.0117, -0.0147,  ...,  0.0519,  0.0538,  0.0548]]]),\n",
       " tensor([[[-0.0880, -0.7188,  0.4376,  ..., -0.2647,  0.3278, -0.1107],\n",
       "          [ 0.3880, -1.2505,  0.6901,  ..., -0.9153,  1.2050, -0.8868],\n",
       "          [ 0.6002, -1.1495,  0.6855,  ..., -0.8445,  0.7359, -0.9922],\n",
       "          ...,\n",
       "          [ 0.3233, -1.3199,  0.0675,  ..., -0.6922,  0.7106, -0.8196],\n",
       "          [ 0.6847, -1.8474,  0.1933,  ..., -1.5083,  1.1304, -0.5578],\n",
       "          [ 0.0479, -0.0499,  0.1923,  ..., -0.0916,  0.1156, -0.0466]]]),\n",
       " tensor([[[ 0.1896, -0.9433,  0.2972,  ..., -0.2829,  0.0919,  0.4495],\n",
       "          [ 0.1530, -0.8414,  0.9008,  ..., -0.6457,  0.1525,  0.1762],\n",
       "          [ 0.4308, -0.9522,  0.4711,  ..., -0.7315,  0.1429, -0.0772],\n",
       "          ...,\n",
       "          [-0.0793, -1.0284,  0.5958,  ..., -0.3046, -0.0589,  0.2305],\n",
       "          [ 0.5603, -1.2828,  0.5530,  ..., -0.9107,  0.2888,  0.4080],\n",
       "          [-0.3142, -0.2400,  0.0867,  ...,  0.1559,  0.1412,  0.2776]]]),\n",
       " tensor([[[ 0.2718, -1.3686,  0.3455,  ..., -1.0038,  0.7322,  0.9942],\n",
       "          [ 0.0463, -1.1734,  0.7015,  ..., -1.1091,  0.9660,  0.8791],\n",
       "          [ 0.2691, -1.4035,  0.3559,  ..., -1.1365,  0.9339,  0.7566],\n",
       "          ...,\n",
       "          [-0.0716, -1.2024,  0.6745,  ..., -1.0392,  0.8887,  0.9510],\n",
       "          [ 0.2544, -1.3477,  0.5265,  ..., -1.2149,  0.9567,  0.9623],\n",
       "          [ 0.0470, -0.7975,  0.3681,  ..., -0.6831,  0.5910,  0.7760]]]),\n",
       " tensor([[[-0.4629, -0.6818,  0.0470,  ..., -0.8973, -0.2850,  1.8897],\n",
       "          [-0.4246, -0.6814,  0.0124,  ..., -0.8831, -0.2882,  1.7592],\n",
       "          [-0.4278, -0.6957,  0.0026,  ..., -0.8803, -0.2894,  1.7500],\n",
       "          ...,\n",
       "          [-0.4823, -0.7262,  0.0560,  ..., -0.8734, -0.2872,  1.7843],\n",
       "          [-0.4329, -0.7042,  0.0658,  ..., -0.8696, -0.2785,  1.8284],\n",
       "          [-0.4415, -0.6507,  0.1466,  ..., -0.7481, -0.4127,  1.7488]]]))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['hidden_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "27cc4b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['hidden_states'][0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "eb6dec24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['hidden_states'][0][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6a04bc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['hidden_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ebc9fb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca88f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "37cafb6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['hidden_states'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223de1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "41e4d342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e6659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f7430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bb7c636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d204f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "539e85aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 33082,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# def get_word_vector(sent, idx, tokenizer, model, layers):\n",
    "encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    " # get all token idxs that belong to the word of interest\n",
    "token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    " \n",
    "with torch.no_grad():\n",
    "     output = model(**encoded)\n",
    "\n",
    "# Get all hidden states\n",
    "states = output.hidden_states\n",
    "# Stack and sum all requested layers\n",
    "output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "# Only select the tokens that constitute the requested word\n",
    "word_tokens_output = output[token_ids_word]\n",
    "\n",
    "word_tokens_output.mean(dim=0)\n",
    "#     return get_hidden_states(encoded, token_ids_word, model, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d62275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "19ae6277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dict['input_ids'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff66788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
